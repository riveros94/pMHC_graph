from core.tracking import save
from Bio.PDB import *
from collections import defaultdict
import numpy as np
from itertools import product
from Bio.PDB.vectors import Vector
from Bio.PDB.Atom import Atom
from Bio.PDB.Residue import Residue
import networkx as nx
import time
# from graphein.protein.edges.distance import compute_distmat
from os import path
impor
t pandas as pd
from collections import Counter
from typing import Any, FrozenSet, Tuple, List, Optional, Union, Dict, Set, Iterable
import itertools
import logging
import matplotlib.pyplot as plt
# from utils.cutils.combinations_filter import filtered_combinations
# from SERD_Addon.classes import StructureSERD
from itertools import combinations, product
from sklearn.metrics.pairwise import cosine_similarity
import json
import math
from statistics import pstdev
import bisect

log = logging.getLogger("CRSProtein")

def save_pdb_with_spheres(atomic_data, selected_residues_data, pdb_filename):
    """
    Salva o arquivo PDB com esferas para os resíduos selecionados.

    Parameters
    ----------
    atomic_data : numpy.ndarray
        Dados atômicos da estrutura.
    selected_residues_data : list
        Lista com os dados dos resíduos selecionados, incluindo coordenadas e profundidade.
    pdb_filename : str
        Nome do arquivo PDB de saída.
    """

    
    with open(pdb_filename, 'w') as f:
        # Adiciona os átomos originais
        for row in atomic_data:
            atom_line = "ATOM  {:5d}  {:<4} {:<3} {:<1} {:>4} {:>8.3f}{:>8.3f}{:>8.3f}{:>6.2f}{:>6.2f}\n".format(
                int(row[0]), row[3], row[2], row[1], int(row[4]), row[5], row[6], row[7], row[8], row[9]
            )
            f.write(atom_line)
        
        # Adiciona as esferas para os resíduos selecionados
        for residue in selected_residues_data:
            residue_number = residue["ResidueNumber"]
            chain = residue["Chain"]
            coordinates = residue["Coordinates"]
            radius = 1.5  # Definindo um raio para a es fera

            sphere_line = "HETATM{:5d}  SPC {:<3} {:<1} {:>4}   {:>8.3f}{:>8.3f}{:>8.3f}  1.00  0.00           SPH\n".format(
                residue_number * 100, "SPC", chain, residue_number, coordinates[0], coordinates[1], coordinates[2]
            )
            f.write(sphere_line)

    print(f"PDB com esferas salvo em {pdb_filename}")

def check_identity(node_pair: Tuple):
    '''
    Takes a tuple of nodes in the following format: 'A:PRO:57', 'A:THR:178')
    '''
    
    if isinstance(node_pair[-1], tuple):
        raise Exception(f"The node of second graph is a tuple {node_pair}")
    
    second_graph_node = node_pair.pop(-1) 
    node_ref = []
    
    while True:
        if isinstance(node_pair[-1], tuple):
            node_pair = node_pair[0]
        elif isinstance(node_pair[-1], str):
            node_ref = node_pair[-1]
            break
        else:
            raise Exception(f"Unexpected type of node. Node: {node_pair[-1]}, type: {type(node_pair[-1])}")
        
    if node_ref.split(":")[1] == node_pair.split(':')[1]:
        return True
    else:
        return False
    
def check_cross_positions(node_pair_pair: Tuple[Tuple, Tuple]):
    """Check if there are cross position of residues between nodes

    Args:
        node_pair_pair (Tuple[Tuple]): A tuple that contain tuple of nodes

    Returns:
        bool: Return true if you don't have cross position between residues
    """
    
    nodeA, nodeB = node_pair_pair[0], node_pair_pair[1]
    setA, setB = set(nodeA), set(nodeB)
    lenA, lenB = len(setA), len(setB)
    not_cross = all(nodeA[k] != nodeB[k] for k in range(len(nodeA))) 
    repeated = (lenA == 1 and lenB != 1) or (lenB == 1 and lenA != 1)
    # permutation = setA == setB
    
    return not_cross  and not repeated

def convert_edges_to_residues(edges: Set[FrozenSet], maps: Dict) -> Tuple[List, List, List]:
    """Convert the edges that contains tuple of indices to tuple of residues

    Args:
        edges (List[Tuple]): A list that contains tuple of edges that are made of tuples of indices
        maps (Dict): A map that relates the indice to residue

    Returns:
        convert_edge (List[Tuple]): Return edges converted to residues notation
    """
    original_edges = []
    edges_indices = []
    converted_edges = []
    residue_maps_unique = maps["residue_maps_unique"]
    possible_nodes_map = maps["possible_nodes"] 
    for edge in edges:
        edge_list = list(edge)
        node1, node2 = possible_nodes_map[edge_list[0]], possible_nodes_map[edge_list[1]]
        converted_node1 = tuple(f"{residue_maps_unique[idx][0]}:{residue_maps_unique[idx][2]}:{residue_maps_unique[idx][1]}" for idx in node1)
        converted_node2 = tuple(f"{residue_maps_unique[idx][0]}:{residue_maps_unique[idx][2]}:{residue_maps_unique[idx][1]}" for idx in node2)

        converted_node1_indice = tuple(idx for idx in node1)
        converted_node2_indice = tuple(idx for idx in node2)
        # if set(converted_node1) != set(converted_node2) and check_cross_positions((converted_node1, converted_node2)):
        # if check_cross_positions((converted_node1, converted_node2)):
        original_edges.append(edge)
        edges_indices.append((converted_node1_indice, converted_node2_indice))
        converted_edges.append((converted_node1, converted_node2))
        # else:
            # log.debug(f'Invalid edge: {edge}, {converted_node1}:{converted_node2}')

    return original_edges, edges_indices, converted_edges
    
def check_multiple_chains(node: Tuple, residue_maps_unique: Dict):
    """Check if at least on node is from different chain

    Args:
        node (tuple): Tuple of residues that together makes a node
        residue_maps_unique (Dict): A dictionary of all residues

    Returns:
        boolean: Return true if there's a different chain in node
    """
    
    chains = set([residue_maps_unique[node_indice][0] for node_indice in node])
    if len(chains) > 1:
        return False
    else:
        return True

def filter_maps_by_nodes(data: dict, 
                        matrices_dict: dict,
                        distance_threshold: float = 10.0, 
                    ) -> Tuple[Dict, Dict]:

    logger = logging.getLogger("association.filter_maps_by_nodes")
    
    contact_maps = data["contact_maps"]
    rsa_maps = data["rsa_maps"]
    residue_maps = data["residue_maps"]
    nodes_graphs = data["nodes_graphs"]

    
    maps = {"full_residue_maps": [], "residue_maps_unique": {}}
    pruned_contact_maps = []
    thresholded_contact_maps = []
    thresholded_rsa_maps = []
    
    for contact_map, rsa_map, residue_map, nodes in zip(
            contact_maps, rsa_maps, residue_maps, nodes_graphs):
         
        indices = []
        
        for node in nodes:
            parts = node.split(":")
            if len(parts) != 3:
                print(f"Node '{node}' does not have three parts separated by ':'")
                continue

            chain, res_name, res_num_str = parts
            key = (chain, res_num_str, res_name)
            if key in residue_map:
                indices.append(residue_map[key])

        pruned_map = contact_map[np.ix_(indices, indices)]
        np.fill_diagonal(pruned_map, np.nan)
        pruned_contact_maps.append(pruned_map)
        
        thresh_map = pruned_map.copy()
        thresh_map[thresh_map >= distance_threshold] = np.nan
        thresholded_contact_maps.append(thresh_map)
        
        thresholded_rsa_maps.append(rsa_map)
         
        full_res_map = {}
        for i, node in enumerate(nodes):
            parts = node.split(":")
            if len(parts) != 3:
                logger.warning(f"Node '{node}' does not have three parts; skipping for full residue map")
                continue

            chain, res_name, res_num_str = parts
            full_res_map[(chain, res_num_str, res_name)] = i
        maps["full_residue_maps"].append(full_res_map)
    
    if matrices_dict is not None:
        matrices_dict["pruned_contact_maps"] = pruned_contact_maps
        matrices_dict["thresholded_contact_maps"] = thresholded_contact_maps
        matrices_dict["thresholded_rsa_maps"] = thresholded_rsa_maps
        matrices_dict["thresholded_depth_maps"] = None
    
    return matrices_dict, maps

def indices_graphs(nodes_lists: List[List]) -> List[Tuple[int, int]]:
    """Make a list that contains indices that indicates the position of each protein in graph

    Args:
        nodes_list (List): A list of protein's resdiues. Each List has their own residues.

    Returns:
        ranges (List[Tuple]): A list of indicesthat indicates the position of each protein in matrix
    """
    
    current = 0
    ranges = []
    for nodes in nodes_lists:
        length = len(nodes)
        ranges.append((current, current + length))
        current += length
    return ranges
    
def create_similarity_matrix(nodes_graphs: list,
                             metadata: dict,
                             residues_factors,
                             similarity_cutoff: float = 0.95,
                             mode="dictionary"):

    total_length = metadata["total_length"]
    ranges_graph = metadata["ranges_graph"]
    
    matrix = np.zeros((total_length, total_length))
    
    for i in range(len(nodes_graphs)):
        for j in range(i + 1, len(nodes_graphs)):
            residuesA = residues_factors[i]
            residuesB = residues_factors[j]
            
            if mode == "dictionary":
                keysA = sorted(residuesA.keys())
                keysB = sorted(residuesB.keys())

                A = np.array([residuesA[k] for k in keysA])
                B = np.array([residuesB[k] for k in keysB])

                similarities = cosine_similarity(A, B)

            elif mode == "1d":
                # def normalize(arr):
                #     return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))

                residuesA = residuesA.reshape(-1, 1)
                residuesB = residuesB.reshape(1, -1)

                similarities = 1 - np.abs(residuesA - residuesB)
            else:
                return None
            startA, endA = ranges_graph[i]
            startB, endB = ranges_graph[j]

            matrix[startA:endA, startB:endB] = similarities
            matrix[startB:endB, startA:endA] = similarities.T
    
    matrix[matrix < similarity_cutoff] = 0
    matrix[matrix >= similarity_cutoff] = 1
    
    return matrix


def value_to_class(
    value: float,
    n_divisions: int,
    threshold: float,
    inverse: bool = False,
    upper_bound: float = 100.0,
    center_tolerance: float = 0.1,  # tolerância ABSOLUTA, em unidades de 'value',
    distances = []
) -> Optional[Union[int, List[int]]]:
    """
    Não-inverse:
      - Domínio: (0, threshold]
      - Bins: (L, R] igualmente espaçados, i = 1..n_divisions, R = i*(threshold/n_divisions)
      - Incerteza: metade do tamanho do bin (len_bin/2)
      - Se |value - centro_da_bin(value)| <= center_tolerance, retorna só [classe_do_value].
      - Caso contrário, retorna todas as classes cujos bins interceptam [value - inc, value + inc].

    Inverse:
      - Igual ao comportamento anterior: mapeia para uma única classe.
    """
    if not inverse:
        if value <= 0 or value > threshold or n_divisions <= 0:
            return None

        length_division = threshold / n_divisions
        if length_division <= 0:
            return None

        i = math.ceil(value / length_division)
        if i < 1:
            i = 1
        if i > n_divisions:
            i = n_divisions

        # Tolerância absoluta para travar no centro
        tol = max(0.0, float(center_tolerance))

        tol = min(tol, (length_division / 2.0) - 1e-12)

        bin_center = (i - 0.5) * length_division
        if abs(value - bin_center) <= tol:
            return [i]

        # Fora da zona central: aplica a incerteza padrão (metade da bin)
        inc = length_division / 2.0
        low = value - inc
        high = value + inc

        classes: List[int] = []
        for j in range(1, n_divisions + 1):
            L = (j - 1) * length_division
            R = j * length_division
            # interseção entre [low, high] e (L, R]
            if (low < R) and (high > L):
                if R <= threshold and high > 0:
                    classes.append(j)

        return classes if classes else None

    # --- inverse inalterado (retorna UMA classe) ---
    else:
        if value < threshold or n_divisions <= 0:
            return None
        span = upper_bound - threshold
        if span <= 0:
            return None
        rel = value - threshold
        length_division = span / n_divisions
        class_name = math.ceil(rel / length_division)
        if class_name < 1:
            class_name = 1
        if class_name > n_divisions:
            class_name = n_divisions
        return class_name

# def value_to_class(value, n_divisions, threshold, inverse=False, upper_bound = 100):

#     if not inverse:
#         if value <= 0 or value > threshold:
#             return None
#         span = threshold
#         rel = value
#     else:
#         if value < threshold:
#             return None

#         span = upper_bound - threshold
#         rel = value - threshold

#     length_division = span / n_divisions
#     class_name = math.ceil(rel / length_division) 
     
#     return class_name

def create_classes_bin(lenght, n_bins): 
    length_division = lenght / n_bins

    return  {
            str(n): ((n-1)*length_division, n*length_division)
            for n in range(n_bins)
        }
 
def find_class(dictionary, value):
    for key in dictionary.keys():
        range_tuple = dictionary[key]
        if range_tuple[0] < value <= range_tuple[1]:
            return key
    else:
        return None

def residue_to_tuple(res):
    res_split = res.split(":")

    return (res_split[0], res_split[2], res_split[1])

def _as_list(x):
    """Converte None -> [], int -> [int], lista/tupla/conjunto -> lista."""
    if x is None:
        return []
    if isinstance(x, (list, tuple, set)):
        return list(x)
    return [x]


def find_triads(graph_data, classes, config, checks):
    G = graph_data["graph"]
    depth = graph_data["residue_depth"]
    rsa = graph_data["rsa"]
    contact_map = graph_data["contact_map"]
    residue_map = graph_data["residue_map_all"]

    triads = {}
    
    if "residues" in classes.keys():
        residue_classes = {
            res: class_name
            for class_name, residues in classes["residues"].items()
            for res in residues
        }
    else:
        residue_classes = None
 
    rsa_classes = classes["rsa"] if "rsa" in classes.keys() else None 
    depth_classes = classes["depth"] if "depth" in classes.keys() else None
    distance_classes = classes["distance"] if "distance" in classes.keys() else None

    n_nodes = len(G.nodes())
    n_edges = len(G.edges())
    
    for center in G.nodes():
        neighbors = {n for n in G.neighbors(center) if n != center}
        for u, w in combinations(neighbors, 2):
            outer_sorted = tuple(sorted([u, w]))
            u_split, center_split, w_split = outer_sorted[0].split(":"), center.split(":"), outer_sorted[1].split(":")
            u_res, center_res, w_res = u_split[1], center_split[1], w_split[1]
            u_tuple, center_tuple, w_tuple = residue_to_tuple(outer_sorted[0]), residue_to_tuple(center), residue_to_tuple(outer_sorted[1])
            u_index, center_index, w_index = residue_map[u_tuple], residue_map[center_tuple], residue_map[w_tuple]
            
            if residue_classes is not None:
                u_res_class, center_res_class, w_res_class = residue_classes[u_res], residue_classes[center_res], residue_classes[w_res] 
            else:
                u_res_class, center_res_class, w_res_class = u_res, center_res, w_res
            
            
            u_resChain = str(u_split[2]) + u_split[0] 
            center_resChain = str(center_split[2]) + center_split[0] 
            w_resChain = str(w_split[2]) + w_split[0] 

            d1 = contact_map[u_index, center_index]
            d2 = contact_map[u_index, w_index]
            d3 = contact_map[center_index, w_index]

            rsa1 = rsa[outer_sorted[0]]*100
            rsa2 = rsa[center]*100
            rsa3 = rsa[outer_sorted[1]]*100

            if checks["depth"]:
                depth1 = depth.loc[depth["ResNumberChain"] == u_resChain]["ResidueDepth"].values[0]
                depth2 = depth.loc[depth["ResNumberChain"] == center_resChain]["ResidueDepth"].values[0]
                depth3 = depth.loc[depth["ResNumberChain"] == w_resChain]["ResidueDepth"].values[0]

                if depth_classes is not None:
                    depth1_class = find_class(depth_classes, depth1)
                    depth2_class = find_class(depth_classes, depth2)
                    depth3_class = find_class(depth_classes, depth3)
                else:
                    depth1_class = value_to_class(depth1, config["depth_bins"], config["depth_filter"])
                    depth2_class = value_to_class(depth2, config["depth_bins"], config["depth_filter"])
                    depth3_class = value_to_class(depth3, config["depth_bins"], config["depth_filter"])
            else:
                depth1_class, depth2_class, depth3_class = 0, 0, 0
                
            if checks["rsa"]:
                if rsa_classes is not None:
                    rsa1_class = find_class(rsa_classes, rsa1)
                    rsa2_class = find_class(rsa_classes, rsa2)
                    rsa3_class = find_class(rsa_classes, rsa3)
                else:
                    rsa1_class = value_to_class(rsa1, config["rsa_bins"], config["rsa_filter"]*100, inverse=True)
                    rsa2_class = value_to_class(rsa2, config["rsa_bins"], config["rsa_filter"]*100, inverse=True)
                    rsa3_class = value_to_class(rsa3, config["rsa_bins"], config["rsa_filter"]*100, inverse=True)
            else:
                rsa1_class, rsa2_class, rsa3_class = 0, 0, 0
                
            # if distance_classes is not None:
            #     d1_class = find_class(distance_classes, d1) 
            #     d2_class = find_class(distance_classes, d2)
            #     d3_class = find_class(distance_classes, d3)
            # else:
            #     d1_class = value_to_class(d1, config["distance_bins"], config["centroid_threshold"])
            #     d2_class = value_to_class(d2, 2*config["distance_bins"], 2*config["centroid_threshold"])
            #     d3_class = value_to_class(d3, config["distance_bins"], config["centroid_threshold"] )
                            
            if distance_classes is not None:
                d1_opts = _as_list(find_class(distance_classes, d1))
                d2_opts = _as_list(find_class(distance_classes, d2))
                d3_opts = _as_list(find_class(distance_classes, d3))
            else:
                d1_opts = _as_list(value_to_class(d1, config["distance_bins"],        config["centroid_threshold"]))
                d2_opts = _as_list(value_to_class(d2, 2*config["distance_bins"], 2*config["centroid_threshold"]))
                d3_opts = _as_list(value_to_class(d3, config["distance_bins"],        config["centroid_threshold"]))

            # Checagem dos descritores que NÃO são distância (não viram lista)
            # print(d1_opts, d1, d2_opts, d2, d3_opts, d3)
            prefix = (depth1_class, depth2_class, depth3_class, rsa1_class, rsa2_class, rsa3_class)
            if None in prefix:
                # algum descriptor obrigatório veio inválido; não há o que inserir
                pass
            else:
                # Se qualquer distância não tem classe possível, não gera combinação
                if d1_opts and d2_opts and d3_opts:
                    for d1_c, d2_c, d3_c in product(d1_opts, d2_opts, d3_opts):
                        full_describer = (d1_c, d2_c, d3_c)
                        full_describer_absolute = (d1, d2, d3)
                        triad = (u_res_class, center_res_class, w_res_class, *full_describer)

                        if triad not in triads:
                            triads[triad] = {
                                "count": 1,
                                "triads_full": [(outer_sorted[0], center, outer_sorted[1], *full_describer)],
                                "triads_absolute": [(outer_sorted[0], center, outer_sorted[1], *full_describer_absolute)]
                            }
                        else:
                            triads[triad]["count"] += 1
                            triads[triad]["triads_full"].append((outer_sorted[0], center, outer_sorted[1], *full_describer))
                            triads[triad]["triads_absolute"].append((outer_sorted[0], center, outer_sorted[1], *full_describer_absolute))
            # else:
                # logging.debug(f"None Found: ({outer_sorted[0], center, outer_sorted[1], d1, d2, d3} | {full_describer}")
    n_triad = 0
    counters = {}
    for triad, data in triads.items():
        n_triad += triads[triad]["count"]

        triads_full = data["triads_absolute"]
        data["triads_absolute_d1"] = sorted(triads_full, key=lambda r: r[-3])

        # if triads[triad]["count"] > 2: print(triad, triads[triad]["count"] )
        if triads[triad]["count"] not in counters.keys():
            counters[triads[triad]["count"]] = 1
        else:
            counters[triads[triad]["count"]] += 1
    
    logging.info(f"N Nodes: {n_nodes} | N Edges: {n_edges} | N Triad: {n_triad} | Unique Triad: {len(triads.keys())}")
    logging.debug(f"Counters: {counters}")

    return triads

def create_residues_classes(path, residues_similarity_cutoff):

    atchley_factors = pd.read_csv(path, index_col = 0)

    sim = cosine_similarity(atchley_factors.values)
    aas = [convert_1aa3aa(aa) for aa in atchley_factors.index.tolist()]
    
    parent = {aa: aa for aa in aas}
    
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(x, y):
        root_x, root_y = find(x), find(y)
        if root_x != root_y:
            parent[root_y] = root_x

    for i in range(len(aas)):
        for j in range(i+1, len(aas)):
            if sim[i, j] >= residues_similarity_cutoff:
                union(aas[i], aas[j])
    
    root_to_class = {}

    residue_to_class = {}
    class_id = 1 
    for aa in aas:
        root = find(aa)
        if root not in root_to_class:
            root_to_class[root] = f"C{class_id}"
            class_id += 1 
        residue_to_class[aa] = root_to_class[root]

    return residue_to_class

ROLE_TO_PAIR_IDX = {
    "UC": (0, 1),  # (u, c)
    "UW": (0, 2),  # (u, w)
    "CW": (1, 2),  # (c, w)
}

# def cross_protein_triads(triads_per_protein, diff):
#     """
#     Parameters
#     ----------
#     triads_per_protein : list[dict]
#         Each dict is the `triads` object you already create for one protein.
#         Keys  -> token tuples, e.g. ('C1','C2','C4',3,10,3)
#         Values -> {"count": int, "triads_full": [triplet tuples …]}

#     Returns
#     -------
#     dict
#         token -> list of combinations, where each combination contains
#                  exactly one element from each protein’s `triads_full`.
#                  Example length‑3 combination:
#                  (
#                    ('A:ALA:15', 'A:LYS:17', 'A:LEU:20', 1, 5, 10),  # protein‑1
#                    ('B:ALA:33', 'B:LYS:35', 'B:LEU:39', 1, 5, 10),  # protein‑2
#                    ('C:ALA:7',  'C:LYS:12', 'C:LEU:19', 1, 5, 10)   # protein‑3
#                  )
#     """
#     # Tokens present in *all* proteins
#     common_tokens = set.intersection(*(set(t.keys()) for t in triads_per_protein))
#     cross_combos = {"full": {}, "partial": {}}
#     for token in common_tokens:
#         cross_combos["full"][token] = set()
#         triad_lists = [prot_triads[token]["triads_absolute_d1"] for prot_triads in triads_per_protein]
    
#         indexed_lists = list(enumerate(triad_lists))
#         ordered_triad_lists =  sorted(indexed_lists, key=lambda x: len(x[1]))

#         refTriad_list = ordered_triad_lists[0][1]
#         for triad in refTriad_list:
#             product_list = []
#             for i in range(1, len(triad_lists)):
#                 index_original = ordered_triad_lists[i][0]
#                 list_i = ordered_triad_lists[i][1]
                
#         cross_combos["full"][token] = set(product(*triad_lists))  # full Cartesian product

     
#     return cross_combos


def _build_index(abs_list):
    """
    abs_list: lista de tuplas (r1, r2, r3, d1, d2, d3)
    Retorna estruturas de índice por d1/d2/d3 e vetor de registros.
    """
    records = list(abs_list)  # ids 0..m-1
    by_d1 = sorted((rec[-3], i) for i, rec in enumerate(records))  # (d1, id)
    by_d2 = sorted((rec[-2], i) for i, rec in enumerate(records))  # (d2, id)
    by_d3 = sorted((rec[-1], i) for i, rec in enumerate(records))  # (d3, id)
    return {"records": records, "by_d1": by_d1, "by_d2": by_d2, "by_d3": by_d3}

def _range_ids(sorted_pairs, lo, hi):
    """
    sorted_pairs: lista ordenada de (chave, id)
    retorna conjunto de ids com chave em [lo, hi]
    """
    keys = [k for k, _ in sorted_pairs]
    L = bisect.bisect_left(keys, lo)
    R = bisect.bisect_right(keys, hi)
    # coletar ids só do range
    return {sorted_pairs[j][1] for j in range(L, R)}

def cross_protein_triads(triads_per_protein, diff):
    """
    triads_per_protein: lista de dicionários por proteína.
      Para um token t:
        triads_per_protein[p][t]["triads_absolute_d1"]  -> lista de (r1,r2,r3,d1,d2,d3)
    diff: tolerância para d1 e d3; para d2 usa-se 2*diff
    """
    import math
    common_tokens = set.intersection(*(set(d.keys()) for d in triads_per_protein))
    cross = {"full": {}} 
    for token in common_tokens:
        abs_lists_len = [len(prot[token]["triads_absolute_d1"]) for prot in triads_per_protein]   
        print(f"Token: {token} | Tamanho cada proteína: {abs_lists_len} | {math.prod(abs_lists_len)}")
    input() 
    for token in common_tokens:
        # obter listas absolutas por proteína para este token
        abs_lists = [prot[token]["triads_absolute_d1"] for prot in triads_per_protein]
        # construir índices por proteína
        idxs = [_build_index(lst) for lst in abs_lists]
        # print(f"idxs: {idxs}")
        sizes = [len(x["records"]) for x in idxs]
        if any(s == 0 for s in sizes):
            continue  

        # escolher a proteína com menor lista como referência
        ref_p = min(range(len(idxs)), key=lambda p: len(idxs[p]["records"]))
        # print(f"Ref_p: {ref_p}")
        ref_idx = idxs[ref_p]
        # print(f"Ref_idx: {ref_idx}")
        other_ps = [p for p in range(len(idxs)) if p != ref_p]

        combos = []  # combinações completas para este token

        for ref_rec in ref_idx["records"]:
            d1_ref, d2_ref, d3_ref = ref_rec[-3], ref_rec[-2], ref_rec[-1]

            w1 = diff
            w2 = 2 * diff
            w3 = diff

            all_candidates = []
            ok = True
            for p in other_ps:
                idx = idxs[p]
                cand1 = _range_ids(idx["by_d1"], d1_ref - w1, d1_ref + w1)

                if not cand1:
                    ok = False
                    break
                cand2 = _range_ids(idx["by_d2"], d2_ref - w2, d2_ref + w2)
                # print(f"cand2: {cand2}")
                if not cand2:
                    ok = False
                    break
                cand3 = _range_ids(idx["by_d3"], d3_ref - w3, d3_ref + w3)
                # print(f"cand3: {cand3}")
                if not cand3:
                    ok = False
                    break

                cands = cand1 & cand2 & cand3  # interseção 3D
                if not cands:
                    ok = False
                    break

                
                # materializar as tuplas absolutas
                all_candidates.append([idx["records"][i] for i in cands])
                # print(f"all_candidates: {all_candidates}")
            # input()
            if not ok:
                continue

            for tail in product(*all_candidates):
                if ref_p == 0:
                    tup = (ref_rec, *tail)
                else:
                    # reconstruir na ordem original das proteínas
                    tmp = [None] * len(idxs)
                    tmp[ref_p] = ref_rec
                    k = 0
                    for p in other_ps:
                        tmp[p] = tail[k]
                        k += 1
                    tup = tuple(tmp)
                combos.append(tup)

        cross["full"][token] = combos

    return cross

def filter_cross_combos_by_distance_variation(
    cross_combos: Dict,
    graph_datas: List[Dict[str, Any]],
    *,
    distance_diff_threshold: float,
    roles_to_check: Iterable[str] = ("UC", "CW"), 
) -> Dict[Tuple, List[Tuple[Tuple, ...]]]:
    """
    Mantém no combo apenas aqueles em que, para cada 'role' verificado,
    (max(distâncias reais) - min(distâncias reais)) <= distance_diff_threshold.

    cross_combos: saída do seu cross_protein_triads (token -> list[combo])
    graph_datas:  lista de graph_data (um por proteína, na MESMA ordem usada no cross_combos)
    """
    roles_to_check = tuple(roles_to_check)
    filtered: Dict[Tuple, List[Tuple[Tuple, ...]]]= {}
    filtered_partial = {}
    cross_combo_len = len(cross_combos["full"].keys())
    compatible = build_token_compatibility(cross_combos["full"])

    full_token = {}
    ti = 1
    tok_len = "\n".join([(token, len(combos)) for token, combos in cross_combos["full"].items()])
    print(tok_len)

    input()
    for token, combos in cross_combos["full"].items():
        print(f"Token: {ti}/{cross_combo_len} | Combos: {len(combos)}")
        full_token[token] = {}
        filtered_partial[token] = set()
        full_partial_tokens = set()
        ti += 1
        kept = []
        combos_len = len(combos)
        tc = 1
        for combo in combos:
            partial_tokens = []
            # print(f"Combo: {tc}/{combos_len}")
            ok = True
            tc += 1
            for role in roles_to_check:
                _subtoken = []
                vals = []
                for k, tri_k in enumerate(combo):
                    a, b = pair_nodes_from_tri_full(tri_k, role)
                    d = pair_distance_from_graph(a, b, graph_datas[k])
                    _subtoken.append((a, b))
                    vals.append(d)

                sigma = pstdev(vals) if len(vals) > 1 else 0.0
                if sigma > distance_diff_threshold:
                    ok = False
                    break
                partial_tokens.append(tuple(_subtoken))
                
            if ok: 
                kept.append(combo)
                full_token[token][combo] = partial_tokens
                full_partial_tokens.union(set(partial_tokens))
        if kept:
            filtered[token] = kept
            filtered_partial[token] = full_partial_tokens
    ttt = ("ALA", "GLU", "LYS", 5, 3, 4)
    # print(cross_combos["full"].keys())
    # print(f'Cross: {cross_combos["full"][ttt]}')
    # print(f'full token combo: {full_token[ttt]}')
    # print(f'Filtered: {filtered[ttt]}')
    # input()
    ti = 1
    for token, combos in filtered.items():
        print(f"[FILTERED] Token: {ti}/{cross_combo_len} | Combo: {len(combos)}")
        compat_tokens = compatible[token]
        partial_tokens = []
        ti += 1
        kept = []
        tc = 1
        for combo in combos:
            ok = False
            part_tokens = full_token[token][combo]
            for part_token in part_tokens:
                for compat_token in compat_tokens:
                    if part_token in filtered_partial[compat_token]:
                        ok = True
                        break
                if ok:
                    break

            if ok:
                kept.append(combo)

        if kept:
            filtered[token] = kept

    return filtered


def association_tokens_for_combo(
    combo: Tuple[Tuple, ...],
    roles_for_tokens: Iterable[str] = ("UC", "CW"),
) -> List[Tuple]:
    """
    Gera tokens de associação robustos para facilitar a reconstrução depois.
    Formato:
      ("UC", ((i1,j1),(i2,j2),...,(iN,jN)))  com (ik,jk) = índices ordenados do par da proteína k
    """
    roles_for_tokens = tuple(roles_for_tokens)
    toks = []
    for role in roles_for_tokens:
        pairs = tuple(pair_indices_from_tri_full(tri_k, role) for tri_k in combo)
        toks.append((role, pairs))
    return toks

def index_filtered_combos_by_association(
    filtered_cross_combos: Dict[Tuple, List[Tuple[Tuple, ...]]],
    *,
    roles_for_tokens: Iterable[str] = ("UC", "CW"),
) -> Dict[Tuple, List[Tuple[Tuple, ...]]]:
    """
    Inverte: anchor_token -> lista de combos aprovados que realizam a mesma associação.
    """
    roles_for_tokens = tuple(roles_for_tokens)
    assoc_index: Dict[Tuple, List[Tuple[Tuple, ...]]]= defaultdict(list)
    for _token, combos in filtered_cross_combos.items():
        for combo in combos:
            for atok in association_tokens_for_combo(combo, roles_for_tokens=roles_for_tokens):
                assoc_index[atok].append(combo)
    return assoc_index

def build_graph_from_filtered_combos(
    filtered_cross_combos: Dict[Tuple, List[Tuple[Tuple, ...]]]
) -> Set[Tuple[Tuple[str, ...], Tuple[str, ...]]]:
    """
    Para cada combo aprovado, cria as arestas (C,U) e (C,W) nos nós alinhados.
    Retorna um set de arestas entre nós-tupla (um rótulo por proteína).
    """
    edges: Set[Tuple[Tuple[str, ...], Tuple[str, ...]]] = set()
    for _token, combos in filtered_cross_combos.items():
        for combo in combos:
            U = tuple(tri[0] for tri in combo)
            C = tuple(tri[1] for tri in combo)
            W = tuple(tri[2] for tri in combo)
            # mesmas duas arestas que você já faz para uma tríade:
            edges.add(tuple(sorted((C, U))))
            edges.add(tuple(sorted((C, W))))
    return edges


def tokens_compatible(t1, t2):
    """
    Each token is (res1, res2, res3, dist12, dist13, dist23).
    Returns True if exactly two residues are shared, both central residues
    (t1[1] and t2[1]) are among those two, and their inter-residue distance matches.
    """
    (A, B, C, d1, d2, d3) = t1
    (X, Y, Z, e1, e2, e3) = t2

    cnt1 = Counter([A, B, C])
    cnt2 = Counter([X, Y, Z])
    shared = list((cnt1 & cnt2).elements())
    if len(shared) != 2:
        return False
    if B not in shared or Y not in shared:
        return False

    # distances must match on the shared pair
    # def pair_dist(a, b, d1, d2, d3):
    #     key = tuple(sorted((a, b)))
    #     return { tuple(sorted((A, B))): d1,
    #              tuple(sorted((A, C))): d2,
    #              tuple(sorted((B, C))): d3 }.get(key)

    def build_map(a, b, c, d1, d2, d3):
        return {
            frozenset((a, b)): d1,
            frozenset((a, c)): d2,
            frozenset((b, c)): d3,
        }

    map1 = build_map(A, B, C, d1, d2, d3)
    map2 = build_map(X, Y, Z, e1, e2, e3)

    key = frozenset(shared)
    return map1.get(key) == map2.get(key)

    # shared_elems = list(shared.elements())
    # key = tuple(sorted(shared_elems))
    # return pair_dist(A, B, d1, d2, d3) == pair_dist(X, Y, e1, e2, e3)

def build_token_compatibility(cross_combos):
    """
    Pattern‐indexed compatibility:

      • We generate 8 “wildcard” patterns per token:
          (resA, resB, *, dAB, *, *), (resB, resA, *, dAB, *, *),
          (*, resA, resB, *, *, dAB), (*, resB, resA, *, *, dAB),
          (resC, resB, *, dBC, *, *), ... etc.

      • We invert them into a pattern→tokens index in O(t·p).

      • For each token, we union its 8 patterns’ token‐lists to get a small
        candidate set, then run your exact `tokens_compatible` only on those.
    """
    tokens = list(cross_combos.keys())
    pattern_map    = defaultdict(set)
    token_patterns = {}

    for t in tokens:
        # A, B, C, _, _, _, _, _, _, d1, _, d3 = t
        # pats = [
        #     (A,    B,    None,  None,   None,   None,   None,   None,   None,   d1, None,   None),
        #     (B,    A,    None,  None,   None,   None,   None,   None,   None,   d1, None,   None),
        #     (None, A,    B,     None,   None,   None,   None,   None,   None,   None,   None,   d1),
        #     (None, B,    A,     None,   None,   None,   None,   None,   None,   None,   None,   d1),

        #     (C,    B,    None,  None,    None,  None,   None,   None,   None,   d3, None, None),
        #     (B,    C,    None,  None,    None,  None,   None,   None,   None,   d3, None, None),
        #     (None, B,    C,     None,    None,  None,   None,   None,   None,   None, None, d3),
        #     (None, C,    B,     None,    None,  None,   None,   None,   None,   None, None, d3),
        # ]
        A, B, C, *rest = t
        d_uc, d_uw, d_cw = rest[-3], rest[-2], rest[-1]
        pats = [
            (A,    B,    None, d_uc),
            (B,    A,    None, d_uc),
            # (A,    None, B,    d_uw),
            # (B,    None, A,    d_uw),
            (None, B,    C,    d_cw),
            (None, C,    B,    d_cw),
        ]
        token_patterns[t] = pats
        for p in pats:
            pattern_map[p].add(t)
    

    compat = defaultdict(list)
    for t1 in tokens:
        candidates = set()
        for p in token_patterns[t1]:
            candidates |= pattern_map[p]
        compat[t1] = [c for c in candidates if c != t1]
    return compat

def build_combos_graph(cross_combos):
    nodes, edges, locator = [], set(), {}
    nid = 0
    
    for token, combos in cross_combos.items():
        log.debug(f"Token: {token} | Combos: {len(combos)}")
        for idx, combo in enumerate(combos):
            locator[(token, idx)] = nid
            nodes.append({"id": nid, "token": token, "combo": combo})
            nid += 1

    log.debug(f"Building Token Compatibility")
    compat = build_token_compatibility(cross_combos)
    log.debug(f"THe build token compatibilited finalized")
    seen = set()

    def shared_distance(tri, idx_pair):
        if idx_pair == (0,1):
            return tri[9]
        elif idx_pair == (1,2):
            return tri[11]
        else:
            raise AssertionError(f"Unexpected shared indices {idx_pair}, center must be shared")

    for t1, comp_list in compat.items():
        for t2 in comp_list:
            if (t2, t1) in seen:
                continue
            seen.add((t1, t2))

            c1_list, c2_list = cross_combos[t1], cross_combos[t2]
            iterator = (
                combinations(range(len(c1_list)), 2)
                if t1 == t2 else product(range(len(c1_list)), range(len(c2_list)))
            )

            for i, j in iterator:
                combo1, combo2 = c1_list[i], c2_list[j]
                first_idx_pair = None
                valid = True

                for tri1, tri2 in zip(combo1, combo2):
                    shared = set(tri1[:3]) & set(tri2[:3])
                    if len(shared) != 2 or tri1[1] not in shared or tri2[1] not in shared:
                        valid = False
                        break

                    idx1 = tuple(sorted(k for k,v in enumerate(tri1[:3]) if v in shared))
                    idx2 = tuple(sorted(k for k,v in enumerate(tri2[:3]) if v in shared))

                    if first_idx_pair is None:
                        first_idx_pair = (idx1, idx2)

                    elif (idx1, idx2) != first_idx_pair:
                        valid = False
                        break

                    d1 = shared_distance(tri1, idx1)
                    d2 = shared_distance(tri2, idx2)
                    if d1 != d2:
                        valid = False
                        break

                if not valid:
                    continue
               
                u1, c1, w1 = [], [], []
                u2, c2, w2 = [], [], []
                for tr1, tr2 in zip(combo1, combo2):
                    u1.append(tr1[0])
                    c1.append(tr1[1])
                    w1.append(tr1[2])

                    u2.append(tr2[0])
                    c2.append(tr2[1])
                    w2.append(tr2[2])
                
                u1, c1, w1 = tuple(u1), tuple(c1), tuple(w1)
                u2, c2, w2 = tuple(u2), tuple(c2), tuple(w2)
                for a, b in ((c1, u1), (c1, w1), (c2, u2), (c2, w2)):
                    if a == b:
                        continue
                    edge = tuple(sorted((a,b)))
                    edges.add(edge)

    # return {"nodes": nodes, "edges": edges}
    return edges


def parse_node(node: str) -> Tuple[str, str, int]:
    chain, res, num = node.split(":")
    try:
        num_i = int(num)
    except ValueError:
        import re
        m = re.match(r"(-?\d+)", num)
        if not m:
            raise
        num_i = int(m.group(1))
    return chain, res, num_i

def pair_distance_from_graph(node_a: str, node_b: str, graph_data: Dict[str, Any]) -> float:
    """
    Recalcula a distância contínua a partir de contact_map/residue_map,
    exatamente como no trecho que você mostrou.
    """
    residue_map = graph_data["residue_map_all"]
    contact_map = graph_data["contact_map"]
    a_tuple = residue_to_tuple(node_a)
    b_tuple = residue_to_tuple(node_b)
    ia = residue_map[a_tuple]
    ib = residue_map[b_tuple]
    return float(contact_map[ia, ib])

def pair_nodes_from_tri_full(tri_full: Tuple, role: str) -> Tuple[str, str]:
    i, j = ROLE_TO_PAIR_IDX[role]
    return tri_full[i], tri_full[j]

def pair_indices_from_tri_full(tri_full: Tuple, role: str) -> Tuple[int, int]:
    a, b = pair_nodes_from_tri_full(tri_full, role)
    _, _, ia = parse_node(a)
    _, _, ib = parse_node(b)
    return (ia, ib) if ia < ib else (ib, ia)


# def parse_node(node: str) -> Tuple[str, str, int]:
#     """
#     Parse a node id of the form 'Chain:RES:ResNum' into (chain, resname, resnum_int).
#     """
#     parts = node.split(":")
#     if len(parts) != 3:
#         raise ValueError(f"Invalid node id: {node}")
#     chain, res, num = parts[0], parts[1], parts[2]
#     try:
#         num_i = int(num)
#     except ValueError:
#         # handle cases like "-1A" where insertion code is attached; keep digits and sign
#         import re
#         m = re.match(r"(-?\d+)", num)
#         if not m:
#             raise
#         num_i = int(m.group(1))
#     return chain, res, num_i


# def find_triads(graph_data: Dict[str, Any],
#                 classes: Dict[str, Any],
#                 config: Dict[str, Any],
#                 checks: Dict[str, bool]):
#     """
#     Extrai tríades por proteína, discretiza descritores (depth, RSA, distâncias)
#     e agrega em um dicionário: token -> {count, triads_full[]}.

#     token = (u_res_class, c_res_class, w_res_class,
#              depth1, depth2, depth3, rsa1, rsa2, rsa3, d_uc, d_uw, d_cw)

#     tri_full = (u_str, c_str, w_str, depth1, depth2, depth3, rsa1, rsa2, rsa3, d_uc, d_uw, d_cw)
#     """
#     G            = graph_data["graph"]
#     depth_df     = graph_data["residue_depth"]
#     rsa          = graph_data["rsa"]
#     contact_map  = graph_data["contact_map"]
#     residue_map  = graph_data["residue_map_all"]

#     triads: Dict[Tuple, Dict[str, Any]] = {}

#     # classes de resíduos (opcional)
#     if "residues" in classes.keys():
#         residue_classes = {
#             res: class_name
#             for class_name, residues in classes["residues"].items()
#             for res in residues
#         }
#     else:
#         residue_classes = None

#     rsa_classes      = classes.get("rsa")
#     depth_classes    = classes.get("depth")
#     distance_classes = classes.get("distance")

#     n_nodes = len(G.nodes())
#     n_edges = len(G.edges())

#     def get_depth_safe(label: str) -> Union[float, None]:
#         # label tipo "42A" (numero+cadeia)
#         series = depth_df.loc[depth_df["ResNumberChain"] == label]["ResidueDepth"]
#         if len(series) == 0:
#             return None
#         return float(series.values[0])

#     for center in G.nodes():
#         neighbors = {n for n in G.neighbors(center) if n != center}
#         for u, w in combinations(neighbors, 2):
#             outer_sorted = tuple(sorted([u, w]))
#             u_split, center_split, w_split = outer_sorted[0].split(":"), center.split(":"), outer_sorted[1].split(":")
#             u_res, center_res, w_res = u_split[1], center_split[1], w_split[1]

#             # índices para contact_map
#             u_tuple      = residue_to_tuple(outer_sorted[0])
#             center_tuple = residue_to_tuple(center)
#             w_tuple      = residue_to_tuple(outer_sorted[1])
#             u_index, center_index, w_index = residue_map[u_tuple], residue_map[center_tuple], residue_map[w_tuple]

#             # classe de resíduo
#             if residue_classes is not None:
#                 u_res_class, center_res_class, w_res_class = residue_classes[u_res], residue_classes[center_res], residue_classes[w_res]
#             else:
#                 u_res_class, center_res_class, w_res_class = u_res, center_res, w_res

#             # chaves para depth
#             u_resChain      = str(u_split[2])      + u_split[0]
#             center_resChain = str(center_split[2]) + center_split[0]
#             w_resChain      = str(w_split[2])      + w_split[0]

#             # distâncias geométricas
#             d_uc = contact_map[u_index, center_index]
#             d_uw = contact_map[u_index, w_index]
#             d_cw = contact_map[center_index, w_index]

#             # RSA (em %)
#             rsa1 = rsa[outer_sorted[0]] * 100.0
#             rsa2 = rsa[center]          * 100.0
#             rsa3 = rsa[outer_sorted[1]] * 100.0

#             # Depth
#             if checks.get("depth", False):
#                 depth1 = get_depth_safe(u_resChain)
#                 depth2 = get_depth_safe(center_resChain)
#                 depth3 = get_depth_safe(w_resChain)
#                 if any(v is None for v in (depth1, depth2, depth3)):
#                     # se faltar depth obrigatória, pula
#                     continue

#                 if depth_classes is not None:
#                     depth1_c = find_class(depth_classes, depth1)
#                     depth2_c = find_class(depth_classes, depth2)
#                     depth3_c = find_class(depth_classes, depth3)
#                 else:
#                     depth1_c = value_to_class(depth1, config["depth_bins"],  config["depth_filter"])
#                     depth2_c = value_to_class(depth2, config["depth_bins"],  config["depth_filter"])
#                     depth3_c = value_to_class(depth3, config["depth_bins"],  config["depth_filter"])
#             else:
#                 depth1_c = depth2_c = depth3_c = 0

#             # RSA discretizada
#             if checks.get("rsa", False):
#                 if rsa_classes is not None:
#                     rsa1_c = find_class(rsa_classes, rsa1)
#                     rsa2_c = find_class(rsa_classes, rsa2)
#                     rsa3_c = find_class(rsa_classes, rsa3)   # <- fix aqui
#                 else:
#                     thr = config["rsa_filter"] * 100.0
#                     # rsa1_c = value_to_class(rsa1, config["rsa_bins"], thr, inverse=True)
#                     # rsa2_c = value_to_class(rsa2, config["rsa_bins"], thr, inverse=True)
#                     # rsa3_c = value_to_class(rsa3, config["rsa_bins"], thr, inverse=True)
#                     rsa1_c, rsa2_c, rsa3_c = 0, 0, 0
#             else:
#                 rsa1_c = rsa2_c = rsa3_c = 0

#             # Distâncias -> listas de opções de classe
#             if distance_classes is not None:
#                 d_uc_opts = _as_list(find_class(distance_classes, d_uc))
#                 d_uw_opts = _as_list(find_class(distance_classes, d_uw))
#                 d_cw_opts = _as_list(find_class(distance_classes, d_cw))
#             else:
#                 d_uc_opts = _as_list(value_to_class(d_uc,        config["distance_bins"],        config["centroid_threshold"]))
#                 d_uw_opts = _as_list(value_to_class(d_uw,   2 *  config["distance_bins"],   2 * config["centroid_threshold"]))
#                 d_cw_opts = _as_list(value_to_class(d_cw,        config["distance_bins"],        config["centroid_threshold"]))

#             prefix = (depth1_c, depth2_c, depth3_c, rsa1_c, rsa2_c, rsa3_c)
#             if None in prefix:
#                 continue
#             if not (d_uc_opts and d_uw_opts and d_cw_opts):
#                 continue

#             for d_uc_c, d_uw_c, d_cw_c in product(d_uc_opts, d_uw_opts, d_cw_opts):
#                 # full_desc = (*prefix, d_uc_c, d_uw_c, d_cw_c)
#                 full_desc = ( d_uc_c, d_uw_c, d_cw_c)
#                 token     = (u_res_class, center_res_class, w_res_class, *full_desc)
#                 tri_full  = (outer_sorted[0], center, outer_sorted[1], *full_desc)
#                 if token not in triads:
#                     triads[token] = {"count": 1, "triads_full": [tri_full]}
#                 else:
#                     triads[token]["count"] += 1
#                     triads[token]["triads_full"].append(tri_full)

#     n_triad = sum(v["count"] for v in triads.values())
#     counters = Counter(v["count"] for v in triads.values())
#     logging.info(f"N Nodes: {n_nodes} | N Edges: {n_edges} | N Triad: {n_triad} | Unique Triad: {len(triads.keys())}")
#     logging.debug(f"Counters: {dict(counters)}")
#     return triads


# # ------------------------------------------------------------------
# # 2) Cross entre proteínas (materializado – útil, mas caro)
# # ------------------------------------------------------------------

# def cross_protein_triads(triads_per_protein: List[Dict]) -> Dict[Tuple, List[Tuple]]:
#     """
#     Interseção de tokens e produto cartesiano das listas triads_full
#     (igual à sua versão, pode explodir – mantido por compatibilidade).
#     """
#     common_tokens = set.intersection(*(set(t.keys()) for t in triads_per_protein))
#     cross_combos = {}
#     for token in common_tokens:
#         triad_lists = [prot_triads[token]["triads_full"] for prot_triads in triads_per_protein]
#         cross_combos[token] = list(product(*triad_lists))
#         # log.debug(f"Cross_combo: {token} | {len(cross_combos[token])}")
#         # input()
#     return cross_combos


# # ------------------------------------------------------------------
# # 3) Compatibilidade de tokens (pré-filtro por padrão com 2 fixos + distância)
# # ------------------------------------------------------------------

# def build_token_compatibility(cross_tokens: Iterable[Tuple]) -> Dict[Tuple, List[Tuple]]:
#     """
#     Gera padrões do tipo (A,B,*,dAB), (B,A,*,dAB), (A,*,B,dAU), (B,*,A,dAU),
#                         (*,B,C,dBC), (*,C,B,dBC).
#     Indexa pattern -> tokens. Para cada token, união dos seus padrões.
#     """
#     tokens = list(cross_tokens)
#     pattern_map: Dict[Tuple, Set[Tuple]] = defaultdict(set)
#     token_patterns: Dict[Tuple, List[Tuple]] = {}

#     for t in tokens:
#         A, B, C, *rest = t
#         d_uc, d_uw, d_cw = rest[-3], rest[-2], rest[-1]
#         pats = [
#             (A,    B,    None, d_uc),
#             (B,    A,    None, d_uc),
#             # (A,    None, B,    d_uw),
#             # (B,    None, A,    d_uw),
#             (None, B,    C,    d_cw),
#             (None, C,    B,    d_cw),
#         ]
#         token_patterns[t] = pats
#         for p in pats:
#             pattern_map[p].add(t)

#     compat: Dict[Tuple, List[Tuple]] = {}
#     for t in tokens:
#         cands = set()
#         for p in token_patterns[t]:
#             cands |= pattern_map[p]
#         cands.discard(t)
#         compat[t] = list(cands)
#     return compat


# # ------------------------------------------------------------------
# # 4) Índices por proteína (par concreto + classe de distância + papel)
# # ------------------------------------------------------------------

# def build_indices_for_protein(triads: Dict[Tuple, Dict[str, Any]]):
#     """
#     Cria índices locais que permitem ir DIRETO aos candidatos que compartilham
#     (i,j) reais e a mesma classe de distância do par compartilhado.

#     Retorno:
#       {
#         "pair_index": Dict[(i,j,d_class,rolepair) -> List[(token, tri_full)]],
#         "token_postings": Dict[token -> Dict[(i,j,d_class,rolepair) -> List[tri_full]]]
#       }
#     rolepair ∈ {"UC","UW","CW"}  identifica qual aresta é o par.
#     """
#     pair_index: Dict[Tuple[int,int,Any,str], List[Tuple[Tuple, Tuple]]] = defaultdict(list)
#     token_postings: Dict[Tuple, Dict[Tuple[int,int,Any,str], List[Tuple]]] = defaultdict(lambda: defaultdict(list))
#     # log.debug(f"Triads: {triads}")
#     for token, payload in triads.items():
#         # log.debug(f"Token: {token} | payload: {payload}")
#         for tri in payload["triads_full"]:
#             u, c, w = tri[0], tri[1], tri[2]
#             _, _, u_id = parse_node(u)
#             _, _, c_id = parse_node(c)
#             _, _, w_id = parse_node(w)

#             d_uc, d_uw, d_cw = tri[-3], tri[-2], tri[-1]

#             keys = [
#                 (min(u_id, c_id), max(u_id, c_id), d_uc, "UC"),
#                 (min(u_id, w_id), max(u_id, w_id), d_uw, "UW"),
#                 (min(c_id, w_id), max(c_id, w_id), d_cw, "CW"),
#             ]
#             for K in keys:
#                 pair_index[K].append((token, tri))
#                 token_postings[token][K].append(tri)
#         # log.debug(f"pair_index: {pair_index}\nToken_postings: {token_postings}")
#         # input()            
#     return {"pair_index": pair_index, "token_postings": token_postings}


# # ------------------------------------------------------------------
# # 5) Grafo de combinações sem explosão: join guiado por índices
# # ------------------------------------------------------------------

# def build_combos_graph_indexed(triads_per_protein: List[Dict[Tuple, Dict[str, Any]]]):
#     """
#     Reescreve a etapa cara usando índices por proteína. Não materializa cross_combos.
#     Para cada par de tokens compatíveis, intersecta diretamente as chaves
#     (i,j,d,rolepair) entre os dois tokens em cada proteína e faz join multi-proteína.

#     Retorna:
#       edges: Set[Tuple[Tuple, Tuple]]
#         Arestas entre blocos (sequências de nós por papel) como no seu build original.
#     """
#     # índices por proteína
#     perP = [build_indices_for_protein(tri) for tri in triads_per_protein]
#     log.debug(f"Fiz as build indices")
#     # universo de tokens e compatibilidade por padrão
#     # all_tokens: Set[Tuple] = set()
#     # for tri in triads_per_protein:
#     #     all_tokens |= set(tri.keys())
#     # compat = build_token_compatibility(all_tokens)
#     common_tokens = set.intersection(*(set(t.keys()) for t in triads_per_protein))
#     compat = build_token_compatibility(common_tokens)
#     log.debug(f"Fiz as compatibilidades | {len(compat.items())}")
#     edges: Set[Tuple[Tuple, Tuple]] = set()

#     for t1, comp_list in compat.items():
#         print(f"COmpat_list: {len(comp_list)}")
#         for t2 in comp_list:
#             perP_pairs: List[List[Tuple[Tuple, Tuple]]] = []
#             possible = True

#             for idxP, idx in enumerate(perP):

#                 postings1 = idx["token_postings"].get(t1, {})
#                 postings2 = idx["token_postings"].get(t2, {})
#                 if not postings1 or not postings2:
#                     possible = False
#                     break
#                 # if idxP == 1:
#                 #     log.debug(f"t1: {t1}, t2: {t2}, comp_list: {comp_list}")
#                 #     # input()
#                 #     log.debug(f"idxP: {idxP}")
#                 #     # input()
#                 #     log.debug(f"Postings 1: {postings1}")
#                 #     log.debug(f"Postings 2: {postings2}")
#                     # input()
#                 # Apenas as chaves (i,j,d,role) que ambos os tokens realizam nessa proteína
#                 common_keys = set(postings1.keys()) & set(postings2.keys())
#                 log.debug(f"{idxP}: Common Keys: {len(common_keys)}")
#                 # if idxP == 1: log.debug(f"Common Keys: {common_keys}")
#                 if not common_keys:
#                     possible = False
#                     break

#                 pairs_here: List[Tuple[Tuple, Tuple]] = []
#                 for K in common_keys:
#                     L1 = postings1[K]
#                     L2 = postings2[K]
#                     print(f"L1: {len(L1)} | L2: {len(L2)}")
#                     # if idxP == 1:
#                     #     log.debug(f"K: {K}\nL1: {L1}\nL2: {L2}")

#                     # Produto LOCAL (listas curtas)
#                     for tri1 in L1:
#                         for tri2 in L2:
#                             pairs_here.append((tri1, tri2))

#                 if not pairs_here:
#                     possible = False
#                     break
                
#                 perP_pairs.append(pairs_here)
#                 log.debug(f"Added: {len(pairs_here)} pairs | New len: {len(perP_pairs)}")
#             if not possible:
#                 continue
#             # log.debug(perP_pairs)
#             # input()
#             # Join multi-proteína somente sobre pares localmente válidos
#             log.debug(f"Per pair: {perP_pairs}")
#             products =  [pair_combo for pair_combo in product(*perP_pairs)]
#             log.debug(f"Products: {len(products)}")
#             # input()
#             for pair_combo in products:
#                 # pair_combo: ((tri1_P1, tri2_P1), (tri1_P2, tri2_P2), ...)
#                 # log.debug(f"Pair Combo: {pair_combo}")
#                 u1, c1, w1 = [], [], []
#                 u2, c2, w2 = [], [], []

#                 for (tri1, tri2) in pair_combo:
#                     u1.append(tri1[0]); c1.append(tri1[1]); w1.append(tri1[2])
#                     u2.append(tri2[0]); c2.append(tri2[1]); w2.append(tri2[2])
#                 # log.debug(f"u1: {u1}, c1: {c1}, w1: {w1}\nu2: {u2}, c2: {c2}, w2: {w2}")
#                 u1, c1, w1 = tuple(u1), tuple(c1), tuple(w1)
#                 u2, c2, w2 = tuple(u2), tuple(c2), tuple(w2)

#                 # mesmas arestas que você monta no build antigo
#                 # edges_add = set()
#                 for a, b in ((c1, u1), (c1, w1), (c2, u2), (c2, w2)):
#                     if a == b:
#                         continue
#                     edge = tuple(sorted((a, b)))
#                     # edges_add.add(edge)
#                     edges.add(edge)
#                 # log.debug(f"Edges adicionadas: {edges_add}")
#     return edges


# # ------------------------------------------------------------------
# # 6) Opcional: checagens de compatibilidade exata entre tokens
# #     (mantidas por completude; o fluxo indexado já garante via (i,j,d,role))
# # ------------------------------------------------------------------

# def tokens_compatible(t1: Tuple, t2: Tuple) -> bool:
#     """
#     Cada token: (res1,res2,res3, dists...)
#     True se compartilham exatamente dois resíduos, ambos os centrais inclusos,
#     e a classe de distância do par compartilhado coincide.
#     """
#     (A, B, C, *rest1) = t1
#     (X, Y, Z, *rest2) = t2
#     d1, d2, d3 = rest1[-3], rest1[-2], rest1[-1]
#     e1, e2, e3 = rest2[-3], rest2[-2], rest2[-1]

#     cnt1 = Counter([A, B, C])
#     cnt2 = Counter([X, Y, Z])
#     shared = list((cnt1 & cnt2).elements())
#     if len(shared) != 2:
#         return False
#     if B not in shared or Y not in shared:
#         return False

#     def build_map(a, b, c, d1, d2, d3):
#         return {
#             frozenset((a, b)): d1,
#             frozenset((a, c)): d2,
#             frozenset((b, c)): d3,
#         }

#     map1 = build_map(A, B, C, d1, d2, d3)
#     map2 = build_map(X, Y, Z, e1, e2, e3)
#     key = frozenset(shared)
#     return map1.get(key) == map2.get(key)

def create_std_matrix(nodes, matrices: dict, maps: dict, threshold: float = 3.0):
    dim = len(nodes[0])
    K = len(nodes) 

    maps_out = {}
    maps_out["possible_nodes"] = {}
    for i, node in enumerate(nodes):
        maps_out["possible_nodes"][i]      = node
        maps_out["possible_nodes"][str(node)] = i
    maps["possible_nodes"] = maps_out["possible_nodes"]
    
    stacked_pruned  = np.empty((dim, K, K))
    stacked_thresh  = np.empty_like(stacked_pruned)

    for p in range(dim):
        idx = [node[p] for node in nodes]

        for i in range(K):
            stacked_pruned[p,  i, :] = matrices["dm_pruned"][idx[i],  idx]
            stacked_thresh[p, i, :]  = matrices["dm_thresholded"][idx[i], idx]

    var_pruned = np.std(stacked_pruned, axis=0)    
    var_thresh = np.std(stacked_thresh, axis=0)  

    mask_pruned = np.any((stacked_pruned == 0) | np.isnan(stacked_pruned), axis=0)
    mask_thresh = np.any((stacked_thresh == 0) | np.isnan(stacked_thresh), axis=0)

    var_pruned = np.where(mask_pruned, np.nan, var_pruned)
    var_thresh = np.where(mask_thresh, np.nan, var_thresh)        

    mask_valid = (0 < var_pruned) & (var_pruned < threshold)
    mask_invalid = ~mask_valid
    var_pruned[mask_valid] = 1
    var_pruned[mask_invalid] = np.nan

    mask_valid = (0 < var_thresh) & (var_thresh < threshold)
    mask_invalid = ~mask_valid
    var_thresh[mask_valid] = 1
    var_thresh[mask_invalid] = np.nan
 
    new_matrices = {
        "dm_possible_nodes": var_pruned,
        "adj_possible_nodes": var_thresh
    }
    

    return new_matrices, maps

def association_product(graph_data: list,
                        config: dict,
                        debug: bool = True) -> Union[Dict[str, List], None]:
    logger = logging.getLogger("association.association_product")
    
    checks = config.get("checks", {"rsa": True, "depth": True})
    classes = config.get("classes", {})


    depths = []
    if checks["depth"]:
        for gd in graph_data:
            df = gd["residue_depth"]
            depth_dict = dict(zip(df["ResNumberChain"], df["ResidueDepth"]))
            depths.append(
                np.array([ depth_dict[node] for node in gd["depth_nodes"] ])
            )

    graph_collection = {
        "graphs": [gd["graph"] for gd in graph_data],
        "triads": [find_triads(gd, classes, config, checks) for gd in graph_data],
        "contact_maps": [gd["contact_map"] for gd in graph_data],
        "residue_maps_all": [gd["residue_map_all"] for gd in graph_data],
        "rsa_maps": [gd["rsa"] for gd in graph_data],
        "nodes_graphs": [sorted(list(gd["graph"].nodes())) for gd in graph_data]
    }

    graph_collection["depths_maps"] = depths
    
    save("association_product", f"graph_collection", graph_collection)
    ranges_graph = indices_graphs(graph_collection["graphs"])
    total_length = sum(len(g.nodes()) for g in graph_collection["graphs"])
    metadata = {
        "total_length": total_length,
        "ranges_graph": ranges_graph
    }
  
    matrices_dict = {
        "type": 0,
        "neighbors": None,
        "rsa": None,
        "identity": None,
        "depth": None,
        "associated": None,
        "similarity": None,
        "dm_thresholded": None,
        "dm_pruned": None,
        "metadata": metadata
    }
    
    filter_input = {
        "contact_maps": graph_collection["contact_maps"],
        "rsa_maps": graph_collection["rsa_maps"],
        "residue_maps": graph_collection["residue_maps_all"],
        "depths_maps": graph_collection["depths_maps"],
        "nodes_graphs": graph_collection["nodes_graphs"]
    }
    
    logger.info("Creating pruned and thresholded arrays...")
    matrices_dict, maps = filter_maps_by_nodes(filter_input,
                                            distance_threshold=config["centroid_threshold"],
                                            matrices_dict=matrices_dict)
    logger.info("Arrays created successfully!")

    current_value = 0
    maps["residue_maps_unique_break"] = {} 
    for i, res_map in enumerate(maps["full_residue_maps"]):
        maps["residue_maps_unique"].update({val + current_value: key for key, val in res_map.items()})
        maps["residue_maps_unique_break"][i] = {val + current_value: key for key, val in res_map.items()}
        current_value += len(res_map)

    inv_maps = {
        k: { res: idx for idx, res in br.items() }
        for k, br in maps["residue_maps_unique_break"].items()
    }

    current_index = 0

    dm_thresh = np.zeros((metadata["total_length"], metadata["total_length"]))
    dm_prune = np.zeros((metadata["total_length"], metadata["total_length"]))

    save("association_product", f"maps", maps)
    save("association_product", f"inv_maps", inv_maps)

    for i, graph in enumerate(graph_collection["graphs"]):

        graph_length = len(graph.nodes())
        new_index = current_index + graph_length
        dm_thresh[current_index:new_index, current_index:new_index] = matrices_dict["thresholded_contact_maps"][i]
        dm_prune[current_index:new_index, current_index:new_index] = matrices_dict["pruned_contact_maps"][i]
        current_index = new_index

    save("association_product", f"dm_thresh", dm_thresh)
    save("association_product", f"dm_prune", dm_prune)
    matrices_dict["dm_thresholded"] = dm_thresh
    matrices_dict["dm_pruned"] = dm_prune

    log.debug(f"Criando cross combos")
    start_cross = time.perf_counter()
    cross_combos = cross_protein_triads(graph_collection["triads"], 1.0)
    end_cross = time.perf_counter()
    log.debug(f"Took {end_cross - start_cross:.6f} seconds to create cross combos")
    filtered_triads = filter_cross_combos_by_distance_variation(
        cross_combos,
        graph_data,
        distance_diff_threshold=2,
        roles_to_check=("UC","CW", "UW")       # recomendo começar com as que passam no centro
    )
    end_filtered = time.perf_counter()
    log.debug(f"Cross Combos finalizado")
    log.debug(f"Took {end_filtered - end_cross:.6f} seconds to Filter graphs")
    # triad_graph = build_combos_graph(cross_combos)
    # triad_graph = build_combos_graph_indexed(graph_collection["triads"])
    triad_graph = build_graph_from_filtered_combos(filtered_triads)
    log.debug(f"Graph finalizado")

    # save("association_product", f"cross_combos", cross_combos)
    save("association_product", f"triad_graph", triad_graph)
    
    tuple_edges = [tuple(edge) for edge in triad_graph]
    # print(tuple_edges[0:20])
    G = nx.Graph()
    G.add_edges_from(tuple_edges)
    log.debug(f"Number of Nodes: {len(G.nodes())} | Number of edges: {len(tuple_edges)}")
    components = list(nx.connected_components(G))
    # components = [component for component in components if len(component) > 3000]
    save("comp_id_0", f"graph_associated_base", G)
    Graphs = [([G], 0)]
    # Graphs = []
    comp_id = 1
    processed_status = 1
    maps["inv_maps"] = inv_maps
    components_len = len(components)
    start_frame = time.perf_counter()
    for component in components:
        len_component = len(component)
        log.debug(f"{comp_id}: Processing component {processed_status} / {components_len} with {len_component} nodes")
        processed_status += 1
        if len_component <= 4: 
            log.debug(f"{comp_id} Skipping component {processed_status-1}, because it has just {len_component} nodes")
            continue
        subG = nx.Graph()
        subG.add_nodes_from(component)

        for u in component:
            for v in G.neighbors(u):
                if v in component:
                    subG.add_edge(u, v)

        dm_thresh_graph = np.zeros((metadata["total_length"], metadata["total_length"]))
        for u, v in subG.edges():
            for p, (res_u, res_v) in enumerate(zip(u, v)):
                if res_u != res_v:
                    split_res_u, split_res_v = res_u.split(":"), res_v.split(":")
                    res_u_tuple = (split_res_u[0], split_res_u[2], split_res_u[1])
                    res_v_tuple = (split_res_v[0], split_res_v[2], split_res_v[1])
                    idx_u = inv_maps[p][res_u_tuple]
                    idx_v = inv_maps[p][res_v_tuple]
                    dm_thresh_graph[idx_u, idx_v] = dm_thresh[idx_u, idx_v]
                    dm_thresh_graph[idx_v, idx_u] = dm_thresh[idx_v, idx_u]

        save(f"comp_id_{comp_id}", f"dm_thresh_graph", dm_thresh)
        matrices_dict["dm_thresholded"] = dm_thresh_graph
        # Graphs.extend([([subG], comp_id)])
        # break
        nodes = list(subG.nodes())
        nodes_indices = []

        for node in nodes:
            node_converted = []
            for k, res in enumerate(node):
                res_split = res.split(":")
                res_tuple = (res_split[0], res_split[2], res_split[1])
                res_indice = inv_maps[k][res_tuple]
                node_converted.append(res_indice)
            nodes_indices.append(node_converted)

        save(f"comp_id_{comp_id}", f"nodes_indices", nodes_indices)

        matrices_mul, maps_mul = create_std_matrix(
            nodes=nodes_indices,
            matrices=matrices_dict,
            maps=maps,
            threshold=config["distance_diff_threshold"]
        )

        save(f"comp_id_{comp_id}", f"matrices_mul", matrices_mul)
        save(f"comp_id_{comp_id}", f"maps_mul", maps_mul)

        frames = generate_frames(
            matrices=matrices_mul,
            maps=maps_mul,
        )

        if len(frames.keys()) > 1:
            Graphs.extend([(create_graph(frames, typeEdge="edges_residues", comp_id=comp_id), comp_id)])
            comp_id += 1
        else:
            log.debug(f"Component Refused")
    end_frame = time.perf_counter()
    log.debug(f"Took {end_frame - start_frame:.6f} seconds to create ALL Frames")
    save("association_product", f"Graphs", Graphs)
    return {
            "AssociatedGraph": Graphs
        }

def generate_frames(matrices, maps, *, debug=True, debug_every=5000):
    """
    Build frames by branching on coherent groups of the frontier.

    dm  : coherence indicator (1 = coherent, else != 1 or NaN)
    adj : adjacency indicator for edges (1 = edge, NaN on diagonal)

    A branch is accepted when the induced subgraph (by adj) over the chosen
    nodes has >= 4 edges and all nodes have degree > 1 within that subgraph.
    """

    dm_raw  = matrices["dm_possible_nodes"].copy()
    adj_raw = matrices["adj_possible_nodes"].copy()
    np.fill_diagonal(dm_raw, 1)
    np.fill_diagonal(adj_raw, np.nan)

    C = (dm_raw == 1)     # Matriz de coerência na forma booleana (os cálculos são mais rápidos)
    A = (adj_raw == 1)    # Matriz de adjacência na forma booleana

    np.fill_diagonal(C, True)
    np.fill_diagonal(A, False)
    K = A.shape[0]

    N_adj = [np.nonzero(A[u])[0] for u in range(K)]  # Vizinhos por adjacência

    frames = {}

    # frame 0 base vetorizado
    ii, jj = np.where(A)
    m = ii < jj
    edges_base = {frozenset((int(i), int(j))) for i, j in zip(ii[m], jj[m])}
    _, edges_idx_0, edges_res_0 = convert_edges_to_residues(edges_base, maps)
    frames[0] = {"edges_indices": edges_idx_0, "edges_residues": edges_res_0}

    residue_maps_unique = maps["residue_maps_unique"]
    possible_nodes_map = maps["possible_nodes"] 

    def frontier_for(chosen: np.ndarray, inter_mask: np.ndarray) -> tuple:
        """
        A função frontier_for constrói a nova fronteira a partir do conjunto de nós já escolhidos.
        A ideia é a seguinte: dado um conjunto de nós escolhidos (chosen), buscamos todos os vizinhos 
        imediatos desses nós no grafo de adjacência A. Porém, precisamos garantir duas coisas:
            1) Nenhum nó que já foi escolhido pode voltar para a fronteira.
            2) Apenas nós que permanecem coerentes com o inter_mask (máscara de coerência acumulada)
            são mantidos na fronteira.
        O resultado é retornado como uma tupla de índices inteiros representando os nós que formam a nova fronteira.
        """
        if chosen.size == 0:
            # Se não há nós escolhidos, a fronteira é vazia
            return ()
        mask = A[chosen].any(axis=0)   # pega todos os vizinhos dos nós escolhidos
        mask[chosen] = False           # remove os próprios escolhidos da fronteira
        mask &= inter_mask             # mantém apenas nós ainda coerentes
        return tuple(np.nonzero(mask)[0].tolist())  # retorna índices dos vizinhos coerentes


    def valid_subgraph(nodes):
        """
        A função valid_subgraph verifica se um conjunto de nós forma um subgrafo válido.
        As condições para validade são:
            1) O subgrafo induzido deve ter pelo menos 4 arestas.
            2) Todos os nós dentro do subgrafo devem ter grau > 1 (ou seja, sem nós pendurados).
        Caso seja válido, retorna (True, es) onde 'es' é o conjunto de arestas representado por pares de nós.
        Caso contrário, retorna (False, set()).
        """
        nodes = np.asarray(sorted(nodes), dtype=np.int32)  # ordena nós para consistência
        sub = A[np.ix_(nodes, nodes)]  # submatriz de adjacência restrita aos nós escolhidos
        deg = sub.sum(axis=1)          # grau de cada nó dentro do subgrafo

        # Verifica se o número de arestas é suficiente (deg.sum() // 2 = nº de arestas)
        # if int(deg.sum() // 2) < 4:
        #     return False, set()

        if np.any(deg < 1):
            return False, set()

        # Extrai as arestas do subgrafo: usamos apenas a parte triangular superior para evitar duplicatas
        ii, jj = np.where(np.triu(sub, 1))
        es = {frozenset((int(nodes[i]), int(nodes[j]))) for i, j in zip(ii, jj)}

        return True, es
    
    # Bron–Kerbosch com pivô, restrito à fronteira usando a matriz de coerência C
    def maximal_cliques(frontier_set):
        """
        Dado um conjunto de nós da fronteira, queremos dividi-los em grupos
        de nós que são todos coerentes entre si (cliques no grafo de coerência).
        Para isso usamos o algoritmo clássico de Bron–Kerbosch com pivô,
        aplicado no subgrafo de coerência induzido pela fronteira.
        """

        # Transformamos a fronteira (set) em array ordenado, para termos consistência.
        F = np.array(sorted(frontier_set), dtype=np.int32)
        if F.size == 0:
            return []

        # fmask marca quais nós fazem parte da fronteira dentro do universo total K
        fmask = np.zeros(K, dtype=bool)
        fmask[F] = True

        # N é um dicionário de vizinhança no grafo de coerência restrito à fronteira:
        # para cada nó u, guardamos apenas os vizinhos coerentes que também estão na fronteira.
        N = {}
        for u in F:
            # nu = todos os nós coerentes com u (linha de C[u])
            nu = np.flatnonzero(C[u])
            # filtramos para manter só os que também estão na fronteira
            nu = nu[fmask[nu]]
            # salvamos como conjunto, excluindo o próprio u
            N[int(u)] = set(int(x) for x in nu if x != u)

        cliques = []

        def bk(R, P, X):
            """
            Bron–Kerbosch recursivo:
            - R = conjunto atual que estamos construindo (candidato a clique)
            - P = candidatos que ainda podem entrar em R
            - X = candidatos já explorados e que não podem mais entrar
            Quando P e X ficam vazios ao mesmo tempo, R é um clique maximal.
            """
            if not P and not X:
                # Encontramos um clique maximal
                cliques.append(tuple(sorted(R)))
                return

            # Escolhemos um pivô (nó com maior vizinhança em N) para reduzir ramificações
            U = P | X
            pivot = max(U, key=lambda v: len(N[v])) if U else None

            # Só vamos expandir pelos nós que NÃO são vizinhos do pivô
            ext = P - (N[pivot] if pivot is not None else set())

            for v in list(ext):
                # Chamamos recursivamente adicionando v ao clique atual
                bk(R | {v}, P & N[v], X & N[v])
                # Após explorar v, movemos ele de P para X (já foi tratado)
                P.remove(v)
                X.add(v)

        # Iniciamos Bron–Kerbosch com:
        # - R vazio (nenhum nó escolhido ainda),
        # - P = todos os nós da fronteira,
        # - X vazio.
        bk(set(), set(int(x) for x in F), set())

        # Ordenamos os cliques encontrados: maiores primeiro
        cliques.sort(key=lambda c: (-len(c), c))
        return cliques


    def mask_signature(mask_bool: np.ndarray) -> bytes:
        # assinatura compacta da máscara boolean
        return np.packbits(mask_bool, bitorder="little").tobytes()

    # expansão unificada: não marca visited aqui
    def expand_groups(chosen, inter_mask, frontier):
        # Achar os grupos de nós coerentes em nossa fronteira é equivalente a converter a nossa fronteira
        # Em um grafo de coerência e então procrurarmos o clique máximo nele
        groups = maximal_cliques(frontier) if frontier else []
        if not groups:
            groups = tuple((v,) for v in sorted(frontier))
        out = []
        for grp in groups:
            inter_g = inter_mask.copy()
            for u in grp:
                inter_g &= C[u]
            chosen_g = tuple(sorted(set(chosen) | set(grp)))
            frontier_g = frontier_for(np.asarray(chosen_g, dtype=np.int32), inter_g)
            sig = (chosen_g, mask_signature(inter_g))
            out.append((sig, chosen_g, inter_g, frontier_g))
        return out

    # ordem de nodes por grau
    deg_all = A.sum(axis=1)
    order_all = np.argsort(-deg_all)

    seen_edge_sets    = set()   # chaves de arestas aceitas
    visited_states    = set()   # (chosen_tuple, mask_signature)
    checked_node_sets = set()   # frozenset(chosen)
    next_frame_id     = 1

    pushes = pops = accepts = 0
    pruned_dupe_state = 0
    max_depth = 0

    for node in order_all: # Percorremos os nós na ordem do maior grau para o menor grau
        
        inter = C[int(node)].copy() # Primeiro começamos pegando a linha de coerência que representa o nó e fazemos uma cópia dela (para não modificar o original)
        accepted = (int(node),) # Iniciamos os nós que serão percorridos com o próprio nó inicial

        neigh = N_adj[int(node)]
        if neigh.size == 0:
            continue

        # filtra vizinhos pela coerência com o node
        # inter[neigh] é bool; pega apenas índices coerentes
        frontier_idx = neigh[inter[neigh]]
        if frontier_idx.size == 0:
            continue

        frontier = set(int(v) for v in frontier_idx) # Lista da fronteira 

        # Vamos criar um stack para realizar um busca em profundidade pelo LIFO (Last In First Out)
        stack = []
        stack_small = []

        """
        Sabemos que dado o nó raíz, nem todos os vizinhos são coerentes entre si, então precisamos separar a fronteira em grupos
        de nós que sejam coerentes entre si. Porém, coerência NÃO é transitiva: um nó u pode ser coerente com w e com v, mas w e v
        não serem coerentes entre si. Para cobrir TODAS as possibilidades válidas sem descartar combinações, construímos um “grafo
        de coerência” sobre a fronteira (arestas indicam dm==1 entre pares de vizinhos) e buscamos cliques máximos nele — cada
        clique representa um subconjunto de vizinhos mutualmente coerentes (par a par).
        
        A função expand_groups() encapsula exatamente isso: dado o conjunto 'accepted' (os nós já escolhidos), a máscara 'inter'
        que representa os nós que permanecem coerentes com TODOS os escolhidos, e a 'frontier' atual (vizinhos no grafo de adjacência
        que ainda sobreviveram na máscara), ela:
          1) monta o grafo de coerência restrito à fronteira atual;
          2) extrai cliques máximos (subgrupos de vizinhos mutuamente coerentes);
          3) para cada clique, produz um próximo estado:
             - 'chosen_g'  = accepted ∪ clique (isto é, avançamos escolhendo este subgrupo coerente);
             - 'inter_g'   = inter multiplicado pelo dm de cada nó do clique (filtrando apenas nós coerentes com TODOS os escolhidos + clique);
             - 'frontier_g'= união dos vizinhos de todos os nós em chosen_g, restrita a quem ainda sobrevive em 'inter_g' e que não está em chosen_g.
        
        O retorno é uma lista de assinaturas 'sig' para deduplicar estados (chosen, máscara, fronteira), e as tuplas (chosen_g, inter_g, frontier_g)
        que formam os filhos do estado atual no backtracking. Dessa forma exploramos ramos por “pacotes coerentes”, sem eliminar à força toda a fronteira.
        """
        groups = expand_groups(accepted, inter, frontier)
        # print(groups)
        for sig, chosen_g, inter_g, frontier_g in groups:
            # Evitamos revisitar exatamente o mesmo estado (mesmo conjunto escolhido, mesma máscara e mesma fronteira),
            # o que previne ciclos e explosão desnecessária de busca.
            if sig in visited_states:
                pruned_dupe_state += 1
                continue
            visited_states.add(sig)

            # Empilhamos o novo estado completo para exploração em profundidade.
            # Guardamos também uma versão “resumida” (sem máscara) só para logs/depuração rápida.
            stack.append((chosen_g, inter_g, frontier_g))
            stack_small.append((chosen_g, frontier_g))
            pushes += 1


        # Log inicial para este nó-semente: quantos estados entraram na pilha e um snapshot compacto.
        # log.debug(f"[node {int(node)}] inicializado com {len(stack)} stack | {stack_small}")

        # Loop principal de busca em profundidade (DFS) sobre os estados empilhados.
        while stack:
            chosen_t, inter_m, frontier_t = stack.pop()
            pops += 1
            chosen = list(chosen_t)
            frontier = set(frontier_t)

            # 1) primeiro tenta expandir (gerar filhos)
            children = []
            groups_s = expand_groups(chosen_t, inter_m, frontier)

            for sig, c_g, m_g, f_g in groups_s:
                if sig in visited_states:
                    pruned_dupe_state += 1
                    continue
                visited_states.add(sig)
                children.append((c_g, m_g, f_g))

            if children:
                # ainda há filhos: empilha e segue a busca
                stack.extend(children)
                pushes += len(children)
            else:
                # 2) nó-folha: avaliamos aceitação do subgrafo
                if len(chosen) >= 4:
                    cn = frozenset(chosen)
                    if cn not in checked_node_sets:
                        checked_node_sets.add(cn)
                        ok, es = valid_subgraph(chosen)
                        if ok:
                            # chave canônica das arestas: pares (u<v), conjunto não-ordenado
                            edge_key = frozenset(tuple(sorted(e)) for e in es)
                            if edge_key not in seen_edge_sets:                       
                                seen_edge_sets.add(edge_key)
                                _, edges_idx, edges_res = convert_edges_to_residues(es, maps)
                                frames[next_frame_id] = {
                                    "edges_indices": edges_idx,
                                    "edges_residues": edges_res,
                                }
                                accepts += 1
                                next_frame_id += 1

            if (pops % debug_every == 0):
                log.debug(
                    f"[progress] pops={pops:,} pushes={pushes:,} accepts={accepts:,} "
                    f"visited={len(visited_states):,} checked={len(checked_node_sets):,} "
                    f"pruned_dupe={pruned_dupe_state:,} stack={len(stack)}"
                )


    # -------- post-pass: keep maximal by edges (subset removal) --------
    others = sorted(
        (fid for fid in frames if fid != 0),
        key=lambda fid: len(frames[fid]["edges_indices"]),
        reverse=True
    )
    ordered = {0: {"edges_indices": [], "edges_residues": []}}
    for new_id, old_id in enumerate(others, start=1):
        ordered[new_id] = frames[old_id]

    final_frames = {}
    kept_edge_sets = []
    fid_out = 0
    for _, fr in ordered.items():
        es = frozenset(map(tuple, fr["edges_indices"]))
        if any(es.issubset(k) for k in kept_edge_sets):
            continue
        # drop strict subsets already kept
        to_drop = [k for k in kept_edge_sets if k.issubset(es) and k != es]
        if to_drop:
            kept_edge_sets = [k for k in kept_edge_sets if k not in to_drop]
        final_frames[fid_out] = fr
        kept_edge_sets.append(es)
        fid_out += 1

    final_frames[0] = frames[0]


    log.debug(f"[done] frames_out={len(final_frames)} "
                f"(accepted_raw={len(others)}, kept_maximal={len(final_frames)-1})")

    return final_frames

def create_graph(edges_dict: Dict, typeEdge: str = "edges_indices", comp_id = 0):
    Graphs = []
    k = 0
    for frame in range(0, len(edges_dict.keys())):
        edges = edges_dict[frame][typeEdge]    
        
        G_sub = nx.Graph()  
        
        if len(edges) > 1:
                
            for sublist in edges:
                sublist = list(sublist)

                node_a = tuple(sublist[0]) if isinstance(sublist[0], np.ndarray) else sublist[0]
                node_b = tuple(sublist[1]) if isinstance(sublist[1], np.ndarray) else sublist[1] 
                G_sub.add_edge(node_a, node_b)
                
            chain_color_map = {}
            color_palette = plt.cm.get_cmap('tab10', 20) 
            color_counter = 1 
            
            if typeEdge == "edges_residues":
                for nodes in G_sub.nodes:
                    chain_id = nodes[0][0]+nodes[1][0]
                    
                    if chain_id not in chain_color_map and chain_id[::-1] not in chain_color_map:
                        chain_color_map[chain_id] = color_palette(color_counter)[:3]
                        chain_color_map[chain_id[::-1]] = chain_color_map[chain_id]  # RGB tuple
                        color_counter += 1

                    G_sub.nodes[nodes]['chain_id'] = chain_color_map[chain_id]
            
            G_sub.remove_nodes_from(list(nx.isolates(G_sub)))
            log.debug(f"{comp_id} Number of nodes graph {k}: {len(G_sub.nodes)}")
            k+= 1

            if k >= 100:
                break
            Graphs.append(G_sub)
    return Graphs

def build_contact_map(
    pdb_file: str,
    *,
    exclude_waters: bool = True,
    atom_preference: Tuple[str, str] = ("CB", "CA"),
    water_atom_preference: Tuple[str, ...] = ("O", "OW", "OH2"),
    fallback_any_atom: bool = True) -> Tuple[np.ndarray, Dict[Tuple[str, int], int], Dict[Tuple[str, int, str], int]]:
    """
    Build a residue–residue distance (contact) map using representative atoms.

    Parameters
    ----------
    pdb_file : str
        Path to a PDB file.
    include_waters : bool, default=False
        If True, water residues are included using an oxygen atom as representative.
    atom_preference : (str, str), default=("CB", "CA")
        Ordered preference of atom names for standard residues.
    water_atom_preference : tuple of str, default=("O", "OW", "OH2")
        Ordered preference of atom names for water residues.
    fallback_any_atom : bool, default=True
        If no preferred atoms are present, fall back to the first atom available.

    Returns
    -------
    contact_map : ndarray of shape (N, N)
        Symmetric matrix of pairwise Euclidean distances (Å) between representative atoms.
    residue_map_dict : dict
        Mapping ``(chain_id, residue_number) -> index``.
    residue_map_dict_all : dict
        Mapping ``(chain_id, residue_number, residue_name) -> index``.

    Notes
    -----
    - Waters are identified by `hetfield == 'W'` or residue name in
      {'HOH', 'H2O', 'WAT', 'TIP3', 'SOL'}.
    - Only the first model is used for deterministic behavior.
    - Residues missing all representative options are skipped.
    """
    from Bio.PDB import PDBParser, MMCIFParser

    WATER_NAMES = {"HOH", "H2O", "WAT", "TIP3", "SOL"}

    suffix = pdb_file.lower()
    if suffix.endswith((".cif", ".mmcif", ".mcif")):
        parser = MMCIFParser(QUIET=True)
    else:
        parser = PDBParser(QUIET=True)

    structure = parser.get_structure("protein", pdb_file)

    # Use only the first model
    model = next(iter(structure))

    entries: List[Tuple[str, str, str, np.ndarray]] = []

    for chain in model:
        chain_id = chain.id
        for residue in chain:
            hetfield, resseq, _icode = residue.id  # (het, number, icode)
            res_name = residue.get_resname().strip()
            is_water = (hetfield == "W") or (res_name in WATER_NAMES)

            # Skip waters if not requested
            if is_water and exclude_waters:
                continue

            coord = None

            if is_water:
                # Prefer oxygen-like atom names for waters
                for atom_name in water_atom_preference:
                    if residue.has_id(atom_name):
                        coord = residue[atom_name].get_coord()
                        break
            else:
                # Standard residues: CB then CA
                for atom_name in atom_preference:
                    if residue.has_id(atom_name):
                        coord = residue[atom_name].get_coord()
                        break

            # Fallback: any atom available (useful for ligands or odd residues)
            if coord is None and fallback_any_atom:
                try:
                    atom = next(residue.get_atoms())
                    coord = atom.get_coord()
                except StopIteration:
                    pass

            if coord is None:
                # No representative atom found; skip this residue
                continue

            residue_full = residue.get_full_id()
            icode = residue_full[-1][-1] 
            res_id = f"{resseq}{icode.strip()}" if icode.strip() else str(resseq)
            entries.append((chain_id, res_id, res_name, np.asarray(coord, dtype=float)))


    # Deterministic ordering: by chain, residue number, residue name
    entries.sort(key=lambda x: (x[0], int(''.join(filter(str.isdigit, x[1]))), ''.join(filter(str.isalpha, x[1])), x[2]))

    if not entries:
        return np.zeros((0, 0), dtype=float), {}, {}

    coords = np.vstack([e[3] for e in entries])  # (N, 3)
    residue_map = [(e[0], e[1]) for e in entries]
    residue_map_all = [(e[0], e[1], e[2]) for e in entries]

    residue_map_dict: Dict[Tuple[str, str], int] = {t: i for i, t in enumerate(residue_map)}
    residue_map_dict_all: Dict[Tuple[str, str, str], int] = {t: i for i, t in enumerate(residue_map_all)}

    # Vectorized pairwise distances
    diff = coords[:, None, :] - coords[None, :, :]
    contact_map = np.sqrt(np.sum(diff * diff, axis=2, dtype=float), dtype=float)

    return contact_map, residue_map_dict, residue_map_dict_all

def create_sphere_residue(residue_name, residue_number, coord):
    atom = Atom("CA", coord, 1.0, 1.0, " ", "DUM", 1, element='CA')
    residue = Residue((' ', int(residue_number), ' '), residue_name, " ")
    residue.add(atom)
    return residue

def align_structures_by_chain(reference_pdb, target_pdb, chain_id):
    """
    Aligns a target structure to a reference structure based on a specified chain ID.

    Args:
        reference_pdb (str): Path to the reference PDB file.
        target_pdb (str): Path to the target PDB file.
        chain_id (str): The chain ID to use for alignment.

    Returns:
        Superimposer: The Superimposer object after alignment.
    """
    # Create a PDB parser
    parser = PDBParser(QUIET=True)

    # Load the structures
    reference_structure = parser.get_structure('reference', reference_pdb)
    target_structure = parser.get_structure('target', target_pdb)

    # Extract the chains
    reference_chain = reference_structure[0][chain_id]
    target_chain = target_structure[0][chain_id]

    # Extract the CA atoms from both chains for alignment
    reference_atoms = [atom for atom in reference_chain.get_atoms() if is_aa(atom.get_parent()) and atom.get_id() == 'CA']
    target_atoms = [atom for atom in target_chain.get_atoms() if is_aa(atom.get_parent()) and atom.get_id() == 'CA']

    if len(reference_atoms) != len(target_atoms):
        raise ValueError("The number of CA atoms in the chains do not match. Ensure both chains have the same number of residues.")

    # Create a Superimposer object
    super_imposer = Superimposer()

    # Set the reference and target atoms for alignment
    super_imposer.set_atoms(reference_atoms, target_atoms)

    # Apply the transformation to the target structure
    super_imposer.apply(target_structure.get_atoms())

    # Print the RMSD
    print(f"RMSD: {super_imposer.rms:.4f} Å")

    return super_imposer

def add_sphere_residues(graphs, list_node_names_mol, output_path, node_name):
    
    for graph, node_names_mol in zip(graphs, list_node_names_mol):
        mol_path = graph[1]
        
        # Read PDB file
        parser = PDBParser()
        structure = parser.get_structure('protein', mol_path)

        # Create a new structure to hold the spheres
        new_structure = Structure.Structure("spheres")
        
        # Create a new model and chain for each mol
        new_model = Model.Model(0)
        new_chain = Chain.Chain('X')
        new_model.add(new_chain)
        new_structure.add(new_model)

        # Keep track of added residues to avoid duplicates
        added_residues = set()

        # Add sphere residues to the new structure
        for residue_info in node_names_mol:
            chain_id, residue_name, residue_number = residue_info.split(':')
            residue_key = (chain_id, int(residue_number))
            if residue_key not in added_residues:
                residue = structure[0][chain_id][int(residue_number)]
                #ca_atom = residue['CA']
                atom_coords = [atom.coord for atom in residue]
                centroid_coords = np.mean(atom_coords, axis=0)
                sphere_residue = create_sphere_residue(residue_name, residue_number, centroid_coords)
                new_chain.add(sphere_residue)
                added_residues.add(residue_key)

        name = mol_path.replace("\\", "_").replace("/", "_")
        # Write the new PDB file
        io = PDBIO()
        io.set_structure(new_structure)
        io.save(path.join(output_path,f'spheres_{name}_{node_name}.pdb'))

def convert_1aa3aa(AA):
    amino_acid_codes = {
    'A': 'ALA',
    'R': 'ARG',
    'N': 'ASN',
    'D': 'ASP',
    'C': 'CYS',
    'E': 'GLU',
    'Q': 'GLN',
    'G': 'GLY',
    'H': 'HIS',
    'I': 'ILE',
    'L': 'LEU',
    'K': 'LYS',
    'M': 'MET',
    'F': 'PHE',
    'P': 'PRO',
    'S': 'SER',
    'T': 'THR',
    'W': 'TRP',
    'Y': 'TYR',
    'V': 'VAL'}
    
    return amino_acid_codes[AA]

def convert_3aa1aa(AA):
    amino_acid_codes = {
    'ALA': 'A',
    'ARG': 'R',
    'ASN': 'N',
    'ASP': 'D',
    'CYS': 'C',
    'GLU': 'E',
    'GLN': 'Q',
    'GLY': 'G',
    'HIS': 'H',
    'ILE': 'I',
    'LEU': 'L',
    'LYS': 'K',
    'MET': 'M',
    'PHE': 'F',
    'PRO': 'P',
    'SER': 'S',
    'THR': 'T',
    'TRP': 'W',
    'TYR': 'Y',
    'VAL': 'V'}

    return amino_acid_codes[AA]

def normalize_rows(arr):
    # Initialize an array to hold the normalized values
    normalized_array = np.zeros_like(arr, dtype=float)
    
    # Iterate over each row
    for i in range(arr.shape[0]):
        row = arr[i, :]
        min_val = np.min(row)
        max_val = np.max(row)
        
        # Apply the normalization formula to each row
        normalized_array[i, :] = (row - min_val) / (max_val - min_val)
        
    return normalized_array

def calculate_atchley_average(node, read_emb):
    # Inicializar um vetor para somar os fatores de Atchley
    atchley_factors_sum = np.zeros(read_emb.shape[1] - 1)  # Exclui a coluna de aminoácidos
    
    if isinstance(node, tuple):
        # Iterar sobre cada átomo no nó (tupla)
        for atom in node:
            residue = convert_3aa1aa(atom.split(":")[1])
            
            # Obter os fatores de Atchley para o resíduo
            atchley_factors = read_emb[read_emb.AA == residue].iloc[:, 1:].values[0]
            
            # Somar os fatores de Atchley
            atchley_factors_sum += atchley_factors
        
        # Calcular a média dos fatores de Atchley
        atchley_factors_avg = atchley_factors_sum / len(node)
    else:
        residue = convert_3aa1aa(node.split(":")[1])
        atchley_factors_avg = read_emb[read_emb.AA == residue].iloc[:, 1:].values[0]
    
    return np.asarray(atchley_factors_avg, dtype=np.float64)

def graph_message_passing(graph, embedding_path, use_degree, norm_features):
    '''
    This function performs a single message passing in the graph nodes to update node features.
    It returns a dictionary with the updated node features.
    '''
    # Obtém a matriz de adjacência
    order = sorted(list(graph.nodes()))
    adj = nx.adjacency_matrix(graph, nodelist=order).todense().astype(np.float64)
    
    # Obtém a matriz de distâncias
    pdb_df = graph.graph["pdb_df"]
    pdb_df = pdb_df.set_index('node_id', inplace=False)
    
    ordered_pdb_df = pdb_df.reindex(order)
    dist_df = compute_distmat(ordered_pdb_df)
    dist_m = np.array(dist_df.values.tolist(), dtype=np.float64)
    
    # Define um epsilon para evitar divisão por zero
    epsilon = 1e-8
    
    # Multiplicação elemento a elemento (Hadamard) e divisão, somando epsilon no denominador
    mult = 1 / (np.array(adj) * dist_m + epsilon)
    mult[mult == np.inf] = 0  # Substitui infinito por 0
    
    # Normaliza as linhas para obter pesos entre 0 e 1
    row_sums = np.sum(mult, axis=1)
    weights_m = mult / row_sums[:, np.newaxis]
    
    # Lê a embedding e constrói a matriz de features
    read_emb = pd.read_csv(embedding_path)
    feature_matrix = np.array([
        read_emb[read_emb.AA == convert_3aa1aa(node.split(":")[1])].iloc[:,1:].values.tolist()[0]
        for node in order
    ])
        
    # Message passing: multiplica a matriz de pesos pela matriz de features
    message_passing_m = weights_m @ feature_matrix
    
    # Concatena as features originais com as atualizadas
    concat_feat_matrix = np.concatenate((feature_matrix, message_passing_m), axis=1)
    
    # Converte a matriz em dicionário usando os nomes dos nós ordenados
    node_names = list(order)
    assert len(node_names) == concat_feat_matrix.shape[0], "Number of keys must match the number of rows in the array."
    
    if use_degree and norm_features:
        # Adiciona a contagem de vizinhos normalizada como feature
        neighbors_count = {node: (len(list(graph.neighbors(node))) - 1) / 10 for node in sorted(list(graph.nodes()))}
        concat_feat_matrix_norm = normalize_rows(concat_feat_matrix)
        feat_MP_dict = {
            node_names[i]: np.concatenate([concat_feat_matrix_norm[i, :], [neighbors_count[node_names[i]]]])
            for i in range(concat_feat_matrix_norm.shape[0])
        }
    else:
        feat_MP_dict = {
            node_names[i]: concat_feat_matrix[i, :]
            for i in range(concat_feat_matrix.shape[0])
        }
    
    return feat_MP_dict

def cosine_similarity2(array1, array2):
    # Ensure the arrays are 1-dimensional
    assert array1.ndim == 1 and array2.ndim == 1, "Arrays must be 1-dimensional"
    
    # Compute the dot product
    dot_product = np.dot(array1, array2)
    
    # Compute the L2 norms (Euclidean norms)
    norm1 = np.linalg.norm(array1)
    norm2 = np.linalg.norm(array2)
    
    # Compute the cosine similarity
    similarity = dot_product / (norm1 * norm2)
    
    return similarity

def check_similarity(node_pair, rep_molA, rep_molB, threshold):
    '''
    Takes a tuple of nodes in the following format: 'A:PRO:57', 'A:THR:178')
    '''
    # print(f"node_pair: {node_pair}\nrep_molA{rep_molA}")
    pairA = "|".join(node_pair[0]) if isinstance(node_pair[0], tuple) else node_pair[0] 
    pairB = "|".join(node_pair[1]) if isinstance(node_pair[1], tuple) else node_pair[1] 
    cos_sim = cosine_similarity(rep_molA[pairA], rep_molB[pairB])
    #debug
    if cos_sim > threshold:
        print(pairA)
        print(pairB)
        print(rep_molA[pairA])
        print(rep_molB[pairB])
        print(cos_sim)

    if cos_sim > threshold:
        return True
    else:
        return False

def get_coords_xyz(node_ID, graph):
    """
    This function gets the ID of a node and a Graphein graph object and returns the xyz coordinates of that node
    """
    pdb_df = graph.graph["pdb_df"]  # Obtém o DataFrame de coordenadas

    # Filtra as linhas onde 'node_id' corresponde ao node_ID
    node_data = pdb_df[pdb_df['node_id'] == node_ID]

    if node_data.empty:
        log.error(f"Node ID {node_ID} não encontrado no pdb_df!")

    # Obtém as coordenadas
    x_coord = node_data['x_coord'].iloc[0]
    y_coord = node_data['y_coord'].iloc[0]
    z_coord = node_data['z_coord'].iloc[0]

    return np.array([x_coord, y_coord, z_coord])

def angle_between_vectors(a, b):
    """
    This function get two vectors and returns the angle betweem them
        """
    # Normalize the vectors
    a_norm = np.linalg.norm(a)
    b_norm = np.linalg.norm(b)

    # Ensure norms are not zero to avoid division by zero
    if a_norm == 0 or b_norm == 0:
        raise ValueError("Vectors must not be zero vectors")

    # Compute dot product and angle
    dot_product = np.dot(a, b)
    cos_angle = dot_product / (a_norm * b_norm)

    # Clip cos_angle to avoid numerical errors
    cos_angle = np.clip(cos_angle, -1.0, 1.0)

    # Return the angle degrees
    return np.degrees(np.arccos(cos_angle))

def unit_vector(vector):
    return vector / np.linalg.norm(vector)

def filter_nodes_angle(G: nx.Graph, graphs: List[nx.Graph], angle_diff: float):
    ang_node_dict = {}  # Dicionário para armazenar as diferenças de ângulos

    log.debug("Filtering angles")
    for node in G.nodes:
        neighbors = list(G.neighbors(node))
        if len(neighbors) < 2:
            continue  # Precisamos de pelo menos 2 vizinhos para calcular ângulos

        ang_diffs = []  # Lista para armazenar as diferenças de ângulo

        # Percorre todos os pares de vizinhos do nó
        for i in range(len(neighbors)):
            for j in range(i + 1, len(neighbors)):
                angles = []  # Lista para armazenar os ângulos de cada grafo

                # Calcula os ângulos para todos os grafos em 'graphs'
                for k, graph in enumerate(graphs):
                    n1, n2 = neighbors[i][k], neighbors[j][k]  # Pega os nós correspondentes no grafo k
                    
                    # Obtém as coordenadas do nó e dos vizinhos
                    coord_node = get_coords_xyz(node[k], graph)
                    coord_n1 = get_coords_xyz(n1, graph)
                    coord_n2 = get_coords_xyz(n2, graph)

                    # Calcula os vetores
                    v1 = coord_n1 - coord_node
                    v2 = coord_n2 - coord_node

                    angle = angle_between_vectors(v1, v2)
                    angles.append(angle)

                for angle1, angle2 in combinations(angles, 2):
                    ang_diffs.append(abs(angle1 - angle2))

        ang_node_dict[node] = ang_diffs

    filtered_nodes_ang = [
        key for key, values in ang_node_dict.items() if all(value < angle_diff for value in values)
    ]

    non_compliant_nodes = [
        key for key, values in ang_node_dict.items() if any(value >= angle_diff for value in values)
    ]

    non_compliant_nodes_sorted = sorted(non_compliant_nodes, key=lambda n: len(G[n]))

    # Cria um dicionário com os nós e seus respectivos vizinhos
    nodes_data = {str(node): list(G.neighbors(node)) for node in non_compliant_nodes_sorted}

    # Salva o dicionário como JSON
    with open("non_compliant_nodes.json", "w", encoding="utf-8") as f:
        json.dump(nodes_data, f, indent=4, ensure_ascii=False)

    return filtered_nodes_ang
